<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on troubles.md</title>
    <link>http://troubles.md/posts/</link>
    <description>Recent content in Posts on troubles.md</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 May 2018 12:19:37 +0200</lastBuildDate>
    
        <atom:link href="http://troubles.md/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>In which the CPU changes my data under my nose</title>
      <link>http://troubles.md/posts/i-triple-equals/</link>
      <pubDate>Tue, 15 May 2018 12:19:37 +0200</pubDate>
      
      <guid>http://troubles.md/posts/i-triple-equals/</guid>
      <description>&lt;p&gt;IEEE 754 has been the source of jokes and confusion for a while because of its &lt;a href=&#34;https://0.30000000000000004.com/&#34;&gt;imprecise representation of decimal numbers&lt;/a&gt;, but the truth is that it&amp;rsquo;s an easily-implementable, efficient representation of floating-point numbers that a lot of applications that don&amp;rsquo;t need high precision couldn&amp;rsquo;t easily do without. It&amp;rsquo;s not without its peculiarities though. Or rather, pecularity singular. Specifically, the existence of NaN. NaN is a weird beast, taking up &lt;a href=&#34;https://en.wikipedia.org/wiki/Single-precision_floating-point_format#Exponent_encoding&#34;&gt;16777208&lt;/a&gt; possible values that could have otherwise been used for real data (although some sneaky people get away with using them &lt;a href=&#34;https://softwareengineering.stackexchange.com/questions/185406/what-is-the-purpose-of-nan-boxing&#34;&gt;for special values&lt;/a&gt;) and &lt;a href=&#34;https://doc.rust-lang.org/std/cmp/trait.PartialOrd.html&#34;&gt;forcing Rust to split its comparison traits into &amp;ldquo;partial&amp;rdquo; and &amp;ldquo;total&amp;rdquo; variants&lt;/a&gt;. A lot of developer time and effort has been spent working out precisely how to handle NaN, and this is the story of my recent personal contribution to that time and effort.&lt;/p&gt;

&lt;p&gt;This particular story about NaN starts with a strange bug in &lt;a href=&#34;https://github.com/paritytech/wasmi&#34;&gt;&lt;code&gt;wasmi&lt;/code&gt;&lt;/a&gt;, our experimental WebAssembly interpreter for blockchains here at Parity. The tests were failing on x86 when compiled for 32-bit, but not for 64-bit. After rewriting our WebAssembly test harness to correctly report failures, I narrowed it down to a few test cases that looked something like the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-lisp&#34; data-lang=&#34;lisp&#34;&gt;(module
  &lt;span style=&#34;color:#75715e&#34;&gt;;; ...&lt;/span&gt;
  (func (export &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;f32.reinterpret_i32&amp;#34;&lt;/span&gt;) (param $x i32)
                                       (result f32)
        (f32.reinterpret/i32 (get_local $x)))
  (func (export &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i32.reinterpret_f32&amp;#34;&lt;/span&gt;) (param $x f32)
                                       (result i32)
        (i32.reinterpret/f32 (get_local $x)))
  &lt;span style=&#34;color:#75715e&#34;&gt;;; ...&lt;/span&gt;
)

&lt;span style=&#34;color:#75715e&#34;&gt;;; ...&lt;/span&gt;

(assert_return (invoke &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;f32.reinterpret_i32&amp;#34;&lt;/span&gt;
                       (i32.const 0x7fa00000))
               (f32.const nan:0x200000)) 
(assert_return (invoke &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;f32.reinterpret_i32&amp;#34;&lt;/span&gt;
                       (i32.const 0xffa00000))
               (f32.const -nan:0x200000))
(assert_return (invoke &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i32.reinterpret_f32&amp;#34;&lt;/span&gt;
                       (f32.const nan:0x200000))
               (i32.const 0x7fa00000)) 
(assert_return (invoke &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i32.reinterpret_f32&amp;#34;&lt;/span&gt;
                       (f32.const -nan:0x200000))
               (i32.const 0xffa00000))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;All other tests were succeeding, but doing a bitwise cast from integer to float and visa-versa was failing. Furthermore, these tests all succeeded on x86_64 (i.e. when compiled for 64-bit), they only failed on x86 (i.e. when compiled for 32-bit). Finally, the difference between the expected value and the received value was only one bit. Instead of returning a float with the bit pattern &lt;code&gt;0x7fa000000&lt;/code&gt; it would return &lt;code&gt;0x7fe000000&lt;/code&gt; instead. If you&amp;rsquo;ve worked with NaNs before, you might have noticed something:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Expected: 0b11111111010000000000000000000000000
Got:      0b11111111110000000000000000000000000
                    ^ This bit is different
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s right, it&amp;rsquo;s the &lt;a href=&#34;https://en.wikipedia.org/wiki/NaN#Signaling_NaN&#34;&gt;quiet bit&lt;/a&gt;! For those unaware, the quiet bit determines whether or not the number should raise an exception when you attempt to use it. This is because the &lt;a href=&#34;https://en.wikipedia.org/wiki/IEEE_754&#34;&gt;IEEE 754 standard&lt;/a&gt; was written at a time when it wasn&amp;rsquo;t necessarily expected that a programming language would include exceptions, and so the standard includes provisions to allow hardware-level exceptions when, for example, dividing by zero. The idea is that when the hardware tries to execute an invalid floating point operation (like division by zero) it is allowed to generate a &amp;ldquo;signalling NaN&amp;rdquo;, or sNaN. This is what we see above, the quiet bit is set to 0, meaning that the NaN is signalling&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. When you try to use this value (for example, by multiplying it with another number) the hardware is allowed to generate an exception and then return a &amp;ldquo;quiet NaN&amp;rdquo;, or qNaN, which has the quiet bit set to 1. I say &amp;ldquo;allowed to&amp;rdquo; rather than &amp;ldquo;does&amp;rdquo; or &amp;ldquo;should&amp;rdquo; because IEEE 754 leaves this unspecified, and in fact as far as I can tell x86 and ARM never generate sNaNs, the only way to create them is to reinterpret an integer with the signalling bit set as a float. Originally, I thought the source of the problem was obvious: &lt;a href=&#34;https://github.com/rust-lang/rust/pull/39271&#34;&gt;Rust was quieting NaN when casting an int to a float&lt;/a&gt;. I remember seeing &lt;a href=&#34;https://github.com/tomaka&#34;&gt;tomaka&lt;/a&gt;&amp;rsquo;s talk at RustFest 2017 on binding C libraries where he mentioned that Rust guarantees that all NaNs are quiet and that you have to explicitly handle that when using floats returned from C (where sNaNs are allowed). So I checked the source of &lt;code&gt;f32::from_bits&lt;/code&gt; and&amp;hellip; &lt;a href=&#34;https://github.com/rust-lang/rust/blob/8010604b2d888ac839147fe27de76cdcc713aa1b/src/libcore/num/f32.rs#L270-L275&#34;&gt;nope&lt;/a&gt;. It just does a normal reintepret. Turn&amp;rsquo;s out tomaka&amp;rsquo;s advice was out-of-date, and Rust &lt;a href=&#34;https://github.com/rust-lang/rust/pull/46012/commits/439576fd7bedf741db5fb6a21c902e858d51f2a0&#34;&gt;now allows sNaNs&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So the hunt was on - where are we operating on this float value? Looking at the code it looked like we just take the number directly from the WebAssembly bytecode and interpret it as a float, no operations required. The bytecode must be correct - after all, the same code compiled for 64-bit worked correctly, and they both read the same bytecode. Since reading the code wasn&amp;rsquo;t helping me at all, I opened up the LLDB debugger&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and attached it to the test binary that is output by &lt;code&gt;cargo test --no-run&lt;/code&gt;. Stepping through functions and checking their return value, I eventually found the culprit:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;run_reinterpret&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;T, U&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; self,
  context: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;mut&lt;/span&gt; FunctionContext
) -&amp;gt; Result&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;InstructionOutcome, TrapKind&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;where&lt;/span&gt;
  RuntimeValue: From&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;U&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;,
  T: &lt;span style=&#34;color:#a6e22e&#34;&gt;FromRuntimeValue&lt;/span&gt;,
  T: &lt;span style=&#34;color:#a6e22e&#34;&gt;TransmuteInto&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;U&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
{
  &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; context
    .value_stack_mut()
    .pop_as::&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;T&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;();

  &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v.transmute_into();
  &lt;span style=&#34;color:#75715e&#34;&gt;//      ^-This call here-^
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
  context.value_stack_mut().push(v.into())&lt;span style=&#34;color:#f92672&#34;&gt;?&lt;/span&gt;;

  Ok(InstructionOutcome::RunNextInstruction)
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That &lt;code&gt;v.transmute_into()&lt;/code&gt; call was returning &lt;code&gt;0x7fe000000&lt;/code&gt; instead of &lt;code&gt;0x7fa000000&lt;/code&gt;! Hang on, like the name implies, &lt;code&gt;v.transmute_into()&lt;/code&gt; should just be a transmute, it should just reinterpret the bits. I looked at the source code, and sure enough:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;impl&lt;/span&gt; TransmuteInto&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;f32&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;i32&lt;/span&gt; {
  &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;transmute_into&lt;/span&gt;(self) -&amp;gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;f32&lt;/span&gt; {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;f32&lt;/span&gt;::from_bits(self)
  }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We&amp;rsquo;re back where we started. As we saw before, &lt;code&gt;f32::from_bits&lt;/code&gt; is just a transmute. Originally I thought it was an old version of the standard library, so I wrote a quick test:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;use test::black_box as bb;

#[test]
fn it_works() {
  assert_eq!(
    bb(0xffa00000),
    bb(
      bb(f32::from_bits(bb(0xffa00000)))
        .to_bits()
    )
  );
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;black_box&lt;/code&gt;/&lt;code&gt;bb&lt;/code&gt; function just ensures that the compiler doesn&amp;rsquo;t optimise anything out. This succeeds, though, even on x86 (i.e. 32-bit, the same architecture where our &lt;code&gt;wasmi&lt;/code&gt; test suite fails). So I checked the disassembly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;core::f32::&amp;lt;impl core::num::Float for f32&amp;gt;::from_bits:
  pushl  %ebp
  movl   %esp,        %ebp
  subl   $0x10,       %esp
  movl   0x8(%ebp),   %eax
  movl   0x8(%ebp),   %ecx
  movl   %ecx,        -0x4(%ebp)
  movss  -0x4(%ebp),  %xmm0
  movl   %eax,        -0xc(%ebp)
  movss  %xmm0,       -0x10(%ebp)
  movss  -0x10(%ebp), %xmm0
  movss  %xmm0,       -0x8(%ebp)
  flds   -0x8(%ebp)
  addl   $0x10,       %esp
  popl   %ebp
  retl   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At first glance, that looks like it&amp;rsquo;s doing a bunch more stuff than just a transmute, but it&amp;rsquo;s actually just moving data about because the test harness is compiled without optimisations by default. If you turn optimisations on, this just compiles to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;core::f32::&amp;lt;impl core::num::Float for f32&amp;gt;::from_bits:
  flds   0x4(%esp)
  retl   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hang on, what&amp;rsquo;s this &lt;code&gt;flds&lt;/code&gt; instruction? We see it in the debug version above, too. It&amp;rsquo;s not there if you disassemble the build for 64-bit:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;core::f32::&amp;lt;impl core::num::Float for f32&amp;gt;::from_bits:
  movd   %edi, %xmm0
  retq   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So what does it do? I&amp;rsquo;m pretty familiar with x86 assembly, but I&amp;rsquo;d never seen this instruction before. Googling it brings up &lt;a href=&#34;https://c9x.me/x86/html/file_module_x86_id_100.html&#34;&gt;this page in the x86 reference&lt;/a&gt;, which says that it pushes the argument onto the FPU register stack. See, what most people call x86 is actually an extension known as &lt;a href=&#34;https://en.wikipedia.org/wiki/X87&#34;&gt;x87&lt;/a&gt;, which implementes floating point operations on top of x86. One common &lt;a href=&#34;https://en.wikipedia.org/wiki/X86_calling_conventions&#34;&gt;calling convention&lt;/a&gt; for x87 is to return integer arguments in &lt;code&gt;%eax&lt;/code&gt; and floating point arguments on the FPU stack, using the &lt;code&gt;fstps&lt;/code&gt; instruction in the code that calls our function in order to load the returned value. This is what &lt;code&gt;flds&lt;/code&gt; is doing here, returning the argument as a float. Ah, but whenever the CPU operates on a float it&amp;rsquo;s allowed to quieten it, and sure enough &lt;code&gt;fld&lt;/code&gt; quietens sNaNs that were passed to it. With LLDB we can read the state of the registers and memory, so let&amp;rsquo;s check out what happens between &lt;code&gt;flds&lt;/code&gt; and &lt;code&gt;fstps&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(lldb) run
## SNIP ##
-&amp;gt;  0x5655b5e0 &amp;lt;+0&amp;gt;: flds   0x4(%esp)
    0x5655b5e4 &amp;lt;+4&amp;gt;: retl   
## SNIP ##
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see we&amp;rsquo;ve stopped on our problematic &lt;code&gt;flds&lt;/code&gt; instruction. Let&amp;rsquo;s check out the value of &lt;code&gt;0x4(%esp)&lt;/code&gt; (i.e. the value of &lt;code&gt;%esp&lt;/code&gt; plus &lt;code&gt;0x4&lt;/code&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(lldb) register read esp
    esp = 0xf77fee3c
(lldb) x/xw &#39;0xf77fee3c + 0x4&#39;
0xf77fee40: 0xffa00000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There we go, that&amp;rsquo;s our signalling NaN. So it&amp;rsquo;s passed into the function OK. Let&amp;rsquo;s see what happens in the calling code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(lldb) si
## SNIP ##
-&amp;gt;  0x5655b55c &amp;lt;+44&amp;gt;: fstps  0x14(%esp)
    0x5655b560 &amp;lt;+48&amp;gt;: addl   $0x18, %esp
    0x5655b563 &amp;lt;+51&amp;gt;: popl   %ebx
    0x5655b564 &amp;lt;+52&amp;gt;: retl   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So we&amp;rsquo;re taking the top of the floating point stack and storing it in &lt;code&gt;0x14(%esp)&lt;/code&gt;. Let&amp;rsquo;s step one more time (to let the &lt;code&gt;fstps&lt;/code&gt; instruction run) and read the value of that memory address:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(lldb) si
## SNIP ##
-&amp;gt;  0x5655b560 &amp;lt;+48&amp;gt;: addl   $0x18, %esp
    0x5655b563 &amp;lt;+51&amp;gt;: popl   %ebx
    0x5655b564 &amp;lt;+52&amp;gt;: retl   
    0x5655b565:       nop    
(lldb) register read esp
     esp = 0xf77fee40
(lldb) x/xw &#39;0xf77fee40 + 0x14&#39;
0xf77fee54: 0xffe00000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There we go, that&amp;rsquo;s a quiet NaN. That&amp;rsquo;s the first time that I&amp;rsquo;ve seen &lt;em&gt;the very act of extracting something to a function&lt;/em&gt; cause tests to fail where they would have otherwise succeeded.&lt;/p&gt;

&lt;p&gt;So the solution? I wrote &lt;a href=&#34;https://github.com/Vurich/nan-preserving-float/blob/master/src/lib.rs&#34;&gt;a wrapper around &lt;code&gt;u32&lt;/code&gt;/&lt;code&gt;u64&lt;/code&gt;&lt;/a&gt; that converts to &lt;code&gt;f32&lt;/code&gt;/&lt;code&gt;f64&lt;/code&gt; when doing operations. This essentially fools the CPU into thinking that these values are integers unless it&amp;rsquo;s doing binary operations on them. Since WebAssembly requires that you quieten NaNs when doing binary operations, this doesn&amp;rsquo;t cause a problem.&lt;/p&gt;

&lt;p&gt;If I&amp;rsquo;ve learned anything from this, it&amp;rsquo;s that no matter how close to the metal you go, you can never truly have total control over the execution of your code. I truly believe that the ability to do CPU-level debugging and to read disassembly is still important even in the age of high-level languages, because &lt;a href=&#34;https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/&#34;&gt;all abstractions are leaky&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Quick note: this is only true for x86, the IEEE 754 spec leaves it unspecified whether 0 or 1 represents signalling, as long as the bit mask is the same (&lt;code&gt;1 &amp;lt;&amp;lt; (sizeof(float) - 6)&lt;/code&gt;). Why they thought it was necessary to leave this unspecified instead of just picking one and sticking with it is beyond me. Maybe on some CPUs it&amp;rsquo;s faster to unset a bit and on others it&amp;rsquo;s faster to set it (let&amp;rsquo;s be real here though, probably not).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Yes, &lt;a href=&#34;https://lldb.llvm.org/&#34;&gt;that&amp;rsquo;s the official name&lt;/a&gt;, assumably DB stands for something other than debugger, like decibel or Deutsche Bahn.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Towards a Brighter Future for Smart Contracts</title>
      <link>http://troubles.md/posts/rust-smart-contracts/</link>
      <pubDate>Mon, 14 May 2018 14:17:23 +0200</pubDate>
      
      <guid>http://troubles.md/posts/rust-smart-contracts/</guid>
      <description>&lt;p&gt;You might have heard recently that here at Parity, our Kovan testnet for Ethereum now supports &lt;a href=&#34;https://paritytech.io/parity-1-10-opportunity-released/&#34;&gt;smart contracts written in WebAssembly&lt;/a&gt;. Deep in the bowels of the Ethereum Foundation&amp;rsquo;s volcano lair, there begins a movement towards &lt;a href=&#34;https://github.com/ewasm/design&#34;&gt;the same ability being integrated into Ethereum proper&lt;/a&gt;. I realise in hindsight that my phrasing would make that a &amp;ldquo;bowel movement&amp;rdquo;, but I&amp;rsquo;m not changing it. Moving on.&lt;/p&gt;

&lt;p&gt;Currently the only feasible way to write smart contracts for Ethereum is to use a language built for EVM. Solidity, Bamboo, Vyper. These are not general-purpose languages that were adapted for Ethereum, they were built from the ground up to be used on Ethereum. This is because the EVM&amp;rsquo;s execution semantics are, to be diplomatic about it, nuts. It bears a lot of the scars of backwards compatibility, and a lot of peculiarities (like 256-bit arithmetic and a limited selection of types) are something that you can&amp;rsquo;t help but expose to the programmer. Hiding it would be too complex and in an environment with the strict limitations of the Ethereum environment. You have to expose as much of the underlying system as possible in order to let the programmer optimise for gas price. Using a general-purpose language would only lead to mismatches between the target VM and the semantics that the language expects.&lt;/p&gt;

&lt;p&gt;However, this has many downsides too - Solidity, the most popular and well-supported EVM language by far, still has tooling support leagues behind general-purpose languages that were created well after it, such as Go and Rust. There exist linters for Solidity, but the rules they have are relatively simple, and many of them are included as standard in the compilers of more popular languages (requiring no external tooling at all). More complex linting rules could be written, but that takes up valuable developer time and as much as we love to pretend that Ethereum is taking over the world, the truth is that the pool of developers willing and able to write complex tooling for EVM languages is small.&lt;/p&gt;

&lt;p&gt;Not only that, but tooling is especially necessary for Solidity, since it is a language with many footguns and awkward features. It can be easy for smart contract developers both new and old to make &lt;a href=&#34;https://paritytech.io/the-multi-sig-hack-a-postmortem/&#34;&gt;very small mistakes&lt;/a&gt; that cause catastrophic damage.&lt;/p&gt;

&lt;p&gt;The ideal situation would be to have the tooling support and professional language design of general-purpose languages for the blockchain, and that&amp;rsquo;s what &lt;a href=&#34;https://webassembly.org/&#34;&gt;WebAssembly&lt;/a&gt; can help provide. WebAssembly is a VM target that (unlike most virtual machines) generally tries to match the semantics of a CPU architecture like ARM or x86 rather than trying to match the semantics of the language that runs on it. As a result, many languages can compile to it unchanged - since most programming languages are built to expose the semantics of the CPU at some level, having the VM emulate a CPU is a good lowest-common-denominator. It also allows us to use world-class optimising compilers that were built for ISAs like x86 and ARM - very important when you&amp;rsquo;re charging by the instruction. &lt;code&gt;solc&lt;/code&gt; has an &lt;code&gt;--optimize&lt;/code&gt; flag but as far as I can tell all that it does is &lt;a href=&#34;https://github.com/ethereum/solidity/blob/develop/libevmasm/JumpdestRemover.cpp&#34;&gt;remove unused jump targets from the list of allowed targets in the assembly&amp;rsquo;s metadata&lt;/a&gt;, which is more of a security feature than an optimisation. Certainly, it&amp;rsquo;s years behind even hobbyist C compilers and decades behind something like LLVM, which often &lt;a href=&#34;http://troubles.md/posts/the-power-of-compilers/&#34;&gt;writes better assembly than a human hand-compiling the same code&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One downside, however, is that these general-purpose languages were not built to write smart contracts in. That, however, can be solved with the right library. Since at Parity we write almost all our code in Rust, you might have gathered that we&amp;rsquo;re quite fond of the language. We&amp;rsquo;ve had experiments in writing smart contracts in Rust for a long time, but the API is still very clunky. You have to manually convert any state that you want to store to and from a 256-bit number (since that&amp;rsquo;s what the EVM stores things as), and since we generate code at compile-time a lot of tools won&amp;rsquo;t work correctly, which negates a large part of the reason that we wanted to use a general-purpose language in the first place! With the current API, to write a simple contract that just does basic arithmetic on a stored number you need to write a lot of code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;pub&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;mod&lt;/span&gt; foo_contract {
  &lt;span style=&#34;color:#66d9ef&#34;&gt;use&lt;/span&gt; pwasm_ethereum;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;use&lt;/span&gt; pwasm_std::&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;use&lt;/span&gt; pwasm_std::hash::H256;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;use&lt;/span&gt; bigint::U256;

  &lt;span style=&#34;color:#66d9ef&#34;&gt;use&lt;/span&gt; pwasm_abi_derive::eth_abi;

  &lt;span style=&#34;color:#66d9ef&#34;&gt;static&lt;/span&gt; STATE_KEY: &lt;span style=&#34;color:#a6e22e&#34;&gt;H256&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; H256([
    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,
    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
  ]);

  &lt;span style=&#34;color:#75715e&#34;&gt;#[eth_abi(FooEndpoint)]&lt;/span&gt;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;pub&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;trait&lt;/span&gt; FooInterface {
    &lt;span style=&#34;color:#e6db74&#34;&gt;/// This is called when the contract gets deployed on-chain
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;constructor&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; self, initial: &lt;span style=&#34;color:#a6e22e&#34;&gt;U256&lt;/span&gt;);
    &lt;span style=&#34;color:#e6db74&#34;&gt;/// Add a number to the total
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;add&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; self, to_add: &lt;span style=&#34;color:#a6e22e&#34;&gt;U256&lt;/span&gt;);
    &lt;span style=&#34;color:#e6db74&#34;&gt;/// Get the total
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; self) -&amp;gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;U256&lt;/span&gt;;
  }

  &lt;span style=&#34;color:#66d9ef&#34;&gt;pub&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;FooContract&lt;/span&gt;;

  &lt;span style=&#34;color:#66d9ef&#34;&gt;impl&lt;/span&gt; FooInterface &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; FooContract {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;constructor&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; self, initial: &lt;span style=&#34;color:#a6e22e&#34;&gt;U256&lt;/span&gt;) {
      pwasm_ethereum::write(
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;STATE_KEY,
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;initial.into()
      );
    }
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;add&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; self, to_add: &lt;span style=&#34;color:#a6e22e&#34;&gt;U256&lt;/span&gt;) {
      &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; current_state: &lt;span style=&#34;color:#a6e22e&#34;&gt;U256&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;
        pwasm_ethereum::read(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;STATE_KEY).into();

      &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; new_state &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; current_state &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; to_add;
      
      pwasm_ethereum::write(
          &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;STATE_KEY, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;new_state.into()
      );
    }

    &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; self) -&amp;gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;U256&lt;/span&gt; {
      pwasm_ethereum::read(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;STATE_KEY).into()
    }
  }
}

&lt;span style=&#34;color:#75715e&#34;&gt;// We have to import a trait in order to use `dispatch`
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// and `dispatch_ctor`.
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;use&lt;/span&gt; pwasm_abi::eth::EndpointInterface;

&lt;span style=&#34;color:#75715e&#34;&gt;#[no_mangle]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;pub&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;call&lt;/span&gt;() {
  &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; endpoint &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; foo_contract::FooEndpoint::new(
      foo_contract::FooContract
  );

  pwasm_ethereum::ret(
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;endpoint.dispatch(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;pwasm_ethereum::input())
  );
}

&lt;span style=&#34;color:#75715e&#34;&gt;#[no_mangle]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;pub&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;deploy&lt;/span&gt;() {
  &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; endpoint &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; foo_contract::FooEndpoint::new(
      foo_contract::FooContract
  );

  endpoint.dispatch_ctor(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;pwasm_ethereum::input());
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is, in a word, awful. The lines of boilerplate outnumber the lines of real logic by more than 10 to 1 (about 3 lines of real logic buried within over 50 lines of boilerplate). It has a bunch of fluff that has no use at all - the &lt;code&gt;self&lt;/code&gt; parameter is never used because all state is read and written using &lt;code&gt;pwasm_ethereum::read&lt;/code&gt; and &lt;code&gt;pwasm_ethereum::write&lt;/code&gt;. You need to write &lt;code&gt;#[no_mangle]&lt;/code&gt; over the &lt;code&gt;call&lt;/code&gt; and &lt;code&gt;deploy&lt;/code&gt; functions. There is no way that you&amp;rsquo;d figure out the &lt;code&gt;pwasm_ethereum::ret(&amp;amp;endpoint.dispatch(&amp;amp;pwasm_ethereum::input()))&lt;/code&gt; incantation without a tutorial, it&amp;rsquo;s totally undiscoverable. You can&amp;rsquo;t get reliable code completion without compiling the entire binary since it uses a procedural macro (which can run arbitrary code). Not only that, but you need the procedural macro because without it you&amp;rsquo;d have no chance of correctly handling the function calls, except that the only thing that the procedural macro actually does for you is dispatch incoming methods to the right place and deserialize input/serialize output. Another frustrating thing is that although Rust has fantastic support for enforcing immutability (much better than Solidity&amp;rsquo;s equivalent), we never use it here. We just take &lt;code&gt;&amp;amp;mut self&lt;/code&gt; for everything and then never use it. This is a great first start, and it&amp;rsquo;s exciting to have Rust code really running on our testnet, but it&amp;rsquo;s got a long way to go before it is usable for real smart contract development by regular people.&lt;/p&gt;

&lt;p&gt;Last week, however, I started sketching up an idea of what a better smart contract API could look like. I had a few goals:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Smart contract code should be easy to read and easy to write.&lt;/li&gt;
&lt;li&gt;We should be able to catch obvious bugs at compile-time, and it should be easy to write a linter for anything that is impossible to catch at compile-time.&lt;/li&gt;
&lt;li&gt;It should compile to very compact code to ensure that it&amp;rsquo;s cheap to deploy smart contracts.&lt;/li&gt;
&lt;li&gt;It should be easy to discover how to use the API even without a tutorial (just using &lt;a href=&#34;https://docs.rs/&#34;&gt;rustdoc&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;We should integrate as well with the rest of the ecosystem as possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With that in mind, here is what I would imagine the previous contract would look like with this new API (the crate name I chose is just &lt;code&gt;pwasm&lt;/code&gt; but that&amp;rsquo;s up for bikeshedding):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#[macro_use]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;extern&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;crate&lt;/span&gt; pwasm;
&lt;span style=&#34;color:#75715e&#34;&gt;#[macro_use]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;extern&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;crate&lt;/span&gt; serde_derive;
&lt;span style=&#34;color:#66d9ef&#34;&gt;extern&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;crate&lt;/span&gt; serde;

&lt;span style=&#34;color:#66d9ef&#34;&gt;use&lt;/span&gt; pwasm::{Contract, ContractDef};

&lt;span style=&#34;color:#75715e&#34;&gt;#[derive(Serialize, Deserialize)]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;pub&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;State&lt;/span&gt; {
  current: &lt;span style=&#34;color:#a6e22e&#34;&gt;U256&lt;/span&gt;,
}

contract&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(foo_contract);

&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;foo_contract&lt;/span&gt;() -&amp;gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;impl&lt;/span&gt; ContractDef&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;State&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; {
  messages&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt; {
    Add(U256);
    Get() -&amp;gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;U256&lt;/span&gt;;
  }

  Contract::new()
    .constructor(&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;_txdata, initial&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; State {
      current: &lt;span style=&#34;color:#a6e22e&#34;&gt;initial&lt;/span&gt;,
    })
    &lt;span style=&#34;color:#75715e&#34;&gt;// These type annotations aren&amp;#39;t necessary, they&amp;#39;re just
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// to show that we don&amp;#39;t receive a mutable reference to the
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// state in the `Get` method.
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    .on_msg_mut::&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Add&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;_env: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;mut&lt;/span&gt; EthEnv, state: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;mut&lt;/span&gt; State, to_add&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; {
      state.current &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; to_add;
    })
    .on_msg::&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Get&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;_env: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;EthEnv&lt;/span&gt;, state: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;State&lt;/span&gt;, ()&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; state.current.clone())
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Much nicer, I think. It may look weird at first since it no longer looks like defining functions on a type, but there&amp;rsquo;s a method to my decision to ditch, um, methods. For a start, using types to represent methods makes it easier to define idiomatic interfaces to external contracts. If an external contract uses a different naming convention to Rust&amp;rsquo;s (which if you&amp;rsquo;re calling a Solidity contract, it probably does) you don&amp;rsquo;t have to have the method names exposed to Rust directly match up with the external contract&amp;rsquo;s. Doing it this way also enables us to better utilise Rust&amp;rsquo;s tooling, we get documentation for free (in the form of &lt;code&gt;rustdoc&lt;/code&gt;), whereas we&amp;rsquo;d have to manually maintain documentation on the syntax for our macro. Adding new features can be done by libraries using the trait system, whereas prodedural macros don&amp;rsquo;t compose by default. For example, you could implement a library for defining contracts as state machines without touching the code of &lt;code&gt;pwasm&lt;/code&gt; itself.&lt;/p&gt;

&lt;p&gt;It uses &lt;code&gt;serde&lt;/code&gt; (the standard serialisation and deserialisation framework for Rust) instead of hand-rolling our own serialisation. Hand-rolling serialisation is fine for our own types, but it means that if we want to allow users to serialise their own types we either need them to write a lot of boilerplate or we need to write an equivalent of &lt;code&gt;serde_derive&lt;/code&gt; for our own serialisation method, duplicating work. It uses Rust&amp;rsquo;s inbuilt mutability guarantees to enforce that we don&amp;rsquo;t do any mutation in the functions that don&amp;rsquo;t need it. It&amp;rsquo;s immutable-by-default, you have to write &lt;code&gt;_mut&lt;/code&gt; if you want a mutable method. This can prevent bugs caused by accidental mutation and can also allow us to avoid spending gas to save the state if we know that the method can&amp;rsquo;t mutate it. The &lt;code&gt;contract&lt;/code&gt; and &lt;code&gt;messages&lt;/code&gt; macros are just &lt;code&gt;macro_rules!&lt;/code&gt; macros, and you can quite simply write contracts without them. They just expand to something like the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// ...
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#[no_mangle]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;__pwasm_ethereum_call&lt;/span&gt;() {
  pwasm::ret(
    foo_contract().call(pwasm::read_state(), pwasm::input())
  );
}

&lt;span style=&#34;color:#75715e&#34;&gt;#[no_mangle]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;__pwasm_ethereum_deploy&lt;/span&gt;() {
  &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; state &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; foo_contract().construct(pwasm::input());
  pwasm::write_state(state);
}

&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;foo_contract&lt;/span&gt;() -&amp;gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;impl&lt;/span&gt; ContractDef&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;State&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; {
  &lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Add&lt;/span&gt;;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Get&lt;/span&gt;;

  &lt;span style=&#34;color:#66d9ef&#34;&gt;impl&lt;/span&gt; pwasm::Message &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; Add {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;type&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Input&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; U256;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;type&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Output&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ();
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; NAME: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;str&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Add&amp;#34;&lt;/span&gt;;
  }

  &lt;span style=&#34;color:#66d9ef&#34;&gt;impl&lt;/span&gt; pwasm::Message &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; Get {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;type&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Input&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ();
    &lt;span style=&#34;color:#66d9ef&#34;&gt;type&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Output&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; U256;
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; NAME: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;str&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Get&amp;#34;&lt;/span&gt;;
  }

  &lt;span style=&#34;color:#75715e&#34;&gt;// ...
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Additionally, the contract&amp;rsquo;s methods take a &lt;code&gt;EthEnv&lt;/code&gt; parameter which represents the blockchain environment itself, and can be used to (for example) get the current block header or send messages to another contract. We can use Rust&amp;rsquo;s mutability guarantees again to enforce that an immutable method can&amp;rsquo;t call a mutable method or suchlike. We also use a simple type-level state machine to make sure you write precisely one constructor (although it&amp;rsquo;s not yet possible in Rust&amp;rsquo;s type system to prevent you writing two handlers for the same message).&lt;/p&gt;

&lt;p&gt;Another benefit of so much being regular Rust code is that you can go to &lt;code&gt;Contract&lt;/code&gt; page in the documentation and quite easily infer how to write a contract from that alone. Since you need to go through the &lt;code&gt;state&lt;/code&gt; parameter to access the contract&amp;rsquo;s state and the &lt;code&gt;env&lt;/code&gt; parameter to access the blockchain, it&amp;rsquo;s easy to work out the complete set of everything a smart contract can do just by going to the documentation page for &lt;code&gt;EthEnv&lt;/code&gt;. There are no top-level functions that manipulate global state and if you get anything wrong the compiler will give you a nice error message.&lt;/p&gt;

&lt;p&gt;Finally, it compiles to code without any unnecessary overhead because it extensively uses Rust&amp;rsquo;s generics and because &lt;code&gt;constructor&lt;/code&gt;, &lt;code&gt;on_msg&lt;/code&gt; and &lt;code&gt;on_msg_mut&lt;/code&gt; are all &lt;code&gt;const fn&lt;/code&gt;s. The API has no overhead compared to writing the verbose version that uses the existing API.&lt;/p&gt;

&lt;p&gt;With this abstraction as a base it should be easy to implement other functionality on top. For example, if you need a database and you don&amp;rsquo;t want to have to serialise and deserialise the entire thing every time you access it (for example, if you&amp;rsquo;re creating an ERC20 token and you want to store the mapping between addresses and balances) you could have a &lt;code&gt;Database&lt;/code&gt; type that internally uses the raw calls to &lt;code&gt;pwasm_ethereum::read&lt;/code&gt; and &lt;code&gt;pwasm_ethereum::write&lt;/code&gt; but externally behaves more like Rust&amp;rsquo;s stdlib &lt;code&gt;HashMap&lt;/code&gt; type. This would also allow you to do things not possible in the current API, like having two databases that can have overlapping keys (by using a unique nonce per instance of the &lt;code&gt;Database&lt;/code&gt; type). That could be implemented as a library on top of the API I&amp;rsquo;ve described here with relatively little code.&lt;/p&gt;

&lt;p&gt;I should say that nothing that you see here is set in stone. I&amp;rsquo;ve just been playing with the idea of writing a new API for smart contracts and I want to get some feedback from real smart contract developers (which I am not one of, I&amp;rsquo;m a Rust core developer) and from other members of the community. I&amp;rsquo;d love to hear your ideas for anything that could be improved here, especially with regards to ergonomics. I&amp;rsquo;m not trying to make it look like Solidity code, I&amp;rsquo;m just trying to make it safe and readable. A simple-working-but-extremely-incomplete implementation that doesn&amp;rsquo;t yet run on the actual blockchain is on my GitHub account &lt;a href=&#34;https://github.com/Vurich/wasm-contract-api&#34;&gt;here&lt;/a&gt; if you would like to see how this could be implemented.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How a Rust upgrade more than tripled the speed of my code</title>
      <link>http://troubles.md/posts/the-power-of-compilers/</link>
      <pubDate>Fri, 11 May 2018 16:07:31 +0200</pubDate>
      
      <guid>http://troubles.md/posts/the-power-of-compilers/</guid>
      <description>&lt;p&gt;I&amp;rsquo;d like to share a quick story about the sheer power of LLVM and the benefits of using higher-level languages over assembly.&lt;/p&gt;

&lt;p&gt;I work at Parity Technologies, who maintains the &lt;a href=&#34;https://github.com/paritytech/parity&#34;&gt;Parity Ethereum client&lt;/a&gt;. In this client we have a need for performant 256-bit arithmetic, which we have to emulate in software since no modern hardware supports it natively.&lt;/p&gt;

&lt;p&gt;For a long time we&amp;rsquo;ve maintained parallel implementations of arithmetic, one in Rust for stable builds and one in inline assembly (which is automatically used when you compile with the nightly compiler). We do this because we store these 256-bit numbers as arrays of 64-bit numbers and there is no way to multiply two 64-bit numbers to get a more-than-64-bit result in Rust (since Rust&amp;rsquo;s integer types only go up to &lt;code&gt;u64&lt;/code&gt;). This is despite the fact that x86_64 (our main target platform) natively supports 128-bit results of calculations with 64-bit numbers. So, we resort to splitting the 64-bit numbers into two 32-bit numbers (because we &lt;em&gt;can&lt;/em&gt; multiply two 32-bit numbers to get a 64-bit result).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;impl&lt;/span&gt; U256 {
  &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;full_mul&lt;/span&gt;(self, other: &lt;span style=&#34;color:#a6e22e&#34;&gt;Self&lt;/span&gt;) -&amp;gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;U512&lt;/span&gt; {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; U256(&lt;span style=&#34;color:#66d9ef&#34;&gt;ref&lt;/span&gt; me) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; U256(&lt;span style=&#34;color:#66d9ef&#34;&gt;ref&lt;/span&gt; you) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; other;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; ret &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;u64&lt;/span&gt;; U512_SIZE];


    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;..U256_SIZE {
      &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; carry &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;u64&lt;/span&gt;;
      &lt;span style=&#34;color:#75715e&#34;&gt;// `split` splits a 64-bit number into upper and lower halves
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;      &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; (b_u, b_l) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; split(you[i]);

      &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;..U256_SIZE {
        &lt;span style=&#34;color:#75715e&#34;&gt;// This process is so slow that it&amp;#39;s faster to check for 0 and skip
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// it if possible.
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; me[j] &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; carry &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; {
          &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; split(me[j]);

          &lt;span style=&#34;color:#75715e&#34;&gt;// `mul_u32` multiplies a 64-bit number that&amp;#39;s been split into
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;          &lt;span style=&#34;color:#75715e&#34;&gt;// an `(upper, lower)` pair by a 32-bit number to get a 96-bit
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;          &lt;span style=&#34;color:#75715e&#34;&gt;// result. Yes, 96-bit (it returns a `(u32, u64)` pair).
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;          &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; (c_l, overflow_l) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mul_u32(a, b_l, ret[i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; j]);

          &lt;span style=&#34;color:#75715e&#34;&gt;// Since we have to multiply by a 64-bit number, we have to do
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;          &lt;span style=&#34;color:#75715e&#34;&gt;// this twice.
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;          &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; (c_u, overflow_u) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mul_u32(a, b_u, c_l &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;);
          ret[i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (c_l &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0xffffffff&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (c_u &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;);

          &lt;span style=&#34;color:#75715e&#34;&gt;// Then we have to do this complex logic to set the result. Gross.
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;          &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; res &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (c_u &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (overflow_u &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;);
          &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; (res, o1) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; res.overflowing_add(overflow_l &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; carry);
          &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; (res, o2) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; res.overflowing_add(ret[i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]);
          ret[i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; res;

          carry &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (o1 &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; o2) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;u64&lt;/span&gt;;
        }
      }
    }

    U512(ret)
  }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You don&amp;rsquo;t even have to understand all of the code to see how non-optimal this is. Inspecting the output of the compiler shows that the generated assembly is extremely suboptimal. It does much more work than necessary essentially just to work around limitations in the Rust language. So we wrote an inline assembly version. The important thing about using inline assembly here is that x86_64 natively supports multiplying two 64-bit values into a 128-bit result. When Rust does &lt;code&gt;a * b&lt;/code&gt; when &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are both &lt;code&gt;u64&lt;/code&gt; the CPU actually multiplies them to create a 128-bit result and then Rust just throws away the upper 64 bits. We want the upper 64 in this case though, and the only way to access it efficiently is by using inline assembly.&lt;/p&gt;

&lt;p&gt;As you can imagine, our assembly implementation was much faster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; name            u64.bench ns/iter  inline_asm.bench ns/iter  diff ns/iter   diff %  speedup 
 u256_full_mul   243,159            197,396                        -45,763  -18.82%   x 1.23 
 u256_mul        268,750            95,843                        -172,907  -64.34%   x 2.80 
 u256_mul_small  1,608              789                               -819  -50.93%   x 2.04 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;u256_full_mul&lt;/code&gt; tests the function above, &lt;code&gt;u256_mul&lt;/code&gt; multiplies two 256-bit numbers to get a 256-bit result (in Rust, we just create a 512-bit result and then throw away the top half but in assembly we have a seperate implementation), and &lt;code&gt;u256_mul_small&lt;/code&gt; multiplies two small 256-bit numbers. As you can see, the assembly implementation is up to 65% faster. This is way, way better. Unfortunately, it only works on nightly, and even then only on x86_64. The truth is that it was a lot of effort and a number of thrown-away implementations to even get the Rust code to &amp;ldquo;only&amp;rdquo; half the speed of the assembly, too. There was simply no good way to give the compiler the information necessary.&lt;/p&gt;

&lt;p&gt;All that changed with &lt;a href=&#34;https://blog.rust-lang.org/2018/05/10/Rust-1.26.html&#34;&gt;Rust 1.26&lt;/a&gt;. Now we can do &lt;code&gt;a as u128 * b as u128&lt;/code&gt; and the compiler will use x86_64&amp;rsquo;s native u64-to-u128 multiplication (even though you cast both numbers to &lt;code&gt;u128&lt;/code&gt; it knows that they&amp;rsquo;re &amp;ldquo;really&amp;rdquo; just &lt;code&gt;u64&lt;/code&gt;, you just want a &lt;code&gt;u128&lt;/code&gt; result). That means our code now looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;impl&lt;/span&gt; U256 {
  &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;full_mul&lt;/span&gt;(self, other: &lt;span style=&#34;color:#a6e22e&#34;&gt;Self&lt;/span&gt;) -&amp;gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;U512&lt;/span&gt; {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; U256(&lt;span style=&#34;color:#66d9ef&#34;&gt;ref&lt;/span&gt; me) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; U256(&lt;span style=&#34;color:#66d9ef&#34;&gt;ref&lt;/span&gt; you) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; other;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; ret &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;u64&lt;/span&gt;; U512_SIZE];

    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;..U256_SIZE {
      &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; carry &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;u64&lt;/span&gt;;
      &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; you[i];

      &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;..U256_SIZE {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; me[j];

        &lt;span style=&#34;color:#75715e&#34;&gt;// This compiles down to just use x86&amp;#39;s native 128-bit arithmetic
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; (hi, low) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; split_u128(a &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; u128 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; b &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; u128);

        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; overflow &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {
          &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; existing_low &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; ret[i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; j];
          &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; (low, o) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; low.overflowing_add(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;existing_low);
          &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;existing_low &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; low;
          o
        };

        carry &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {
          &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; existing_hi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; ret[i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;];
          &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; hi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; hi &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; overflow &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;u64&lt;/span&gt;;
          &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; (hi, o0) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; hi.overflowing_add(carry);
          &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; (hi, o1) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; hi.overflowing_add(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;existing_hi);
          &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;existing_hi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; hi;

          (o0 &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; o1) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;u64&lt;/span&gt;
        }
      }
    }

    U512(ret)
  }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Although it&amp;rsquo;s almost certainly not as fast as using the LLVM-native &lt;code&gt;i256&lt;/code&gt; type, the speed is much, much better. Here it is compared to the original Rust implementation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; name            u64.bench ns/iter  u128.bench ns/iter  diff ns/iter   diff %  speedup 
 u256_full_mul   243,159            73,416                  -169,743  -69.81%   x 3.31 
 u256_mul        268,750            85,797                  -182,953  -68.08%   x 3.13 
 u256_mul_small  1,608              558                       -1,050  -65.30%   x 2.88 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which is great, we now get a speed boost on stable. Since we only compile the binaries for the Parity client on stable the only people who could use the assembly before were those who compiled from source, so this is an improvement for a lot of users. But wait, there&amp;rsquo;s more! The new compiled code actually manages to beat the assembly implementation by a significant margin, even beating the assembly on the benchmark that multiplies two 256-bit numbers to get a 256-bit result. This is despite the fact that the Rust code still produces a 512-bit result first and then discards the upper half, where the assembly implementation does not:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; name            inline_asm.bench ns/iter  u128.bench ns/iter  diff ns/iter   diff %  speedup 
 u256_full_mul   197,396                   73,416                  -123,980  -62.81%   x 2.69 
 u256_mul        95,843                    85,797                   -10,046  -10.48%   x 1.12 
 u256_mul_small  789                       558                         -231  -29.28%   x 1.41 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the full multiplication that&amp;rsquo;s an absolutely massive improvement, especially since the original code used highly-optimised assembly incantations from our resident cycle wizard. Here&amp;rsquo;s where the faint of heart might want to step out for a moment, because I&amp;rsquo;m about to dive into the generated assembly.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the hand-written assembly. I&amp;rsquo;ve presented it without comment because I want to comment the assembly that is actually emitted by the compiler (since, as you&amp;rsquo;ll see, the &lt;code&gt;asm!&lt;/code&gt; macro hides more than you&amp;rsquo;d expect):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;impl&lt;/span&gt; U256 {
  &lt;span style=&#34;color:#e6db74&#34;&gt;/// Multiplies two 256-bit integers to produce full 512-bit integer
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;/// No overflow possible
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;pub&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;full_mul&lt;/span&gt;(self, other: &lt;span style=&#34;color:#a6e22e&#34;&gt;U256&lt;/span&gt;) -&amp;gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;U512&lt;/span&gt; {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; self_t: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;[&lt;span style=&#34;color:#66d9ef&#34;&gt;u64&lt;/span&gt;; &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;self.&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; other_t: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;[&lt;span style=&#34;color:#66d9ef&#34;&gt;u64&lt;/span&gt;; &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;other.&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; result: [&lt;span style=&#34;color:#66d9ef&#34;&gt;u64&lt;/span&gt;; &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;unsafe&lt;/span&gt; { ::core::mem::uninitialized() };
    &lt;span style=&#34;color:#66d9ef&#34;&gt;unsafe&lt;/span&gt; {
      asm&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $8, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $12
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov %rax, $0
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov %rdx, $1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $8, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $13
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        add %rax, $1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, %rdx
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov %rdx, $2
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $8, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $14
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        add %rax, $2
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, %rdx
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov %rdx, $3
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $8, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $15
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        add %rax, $3
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, %rdx
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov %rdx, $4
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $9, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $12
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        add %rax, $1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc %rdx, $2
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $3
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $4
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        xor $5, $5
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $5
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        xor $6, $6
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $6
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        xor $7, $7
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $7
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $9, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $13
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        add %rax, $2
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc %rdx, $3
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $4
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $5
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $6
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $7
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $9, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $14
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        add %rax, $3
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc %rdx, $4
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $5
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $6
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $7
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $9, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $15
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        add %rax, $4
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc %rdx, $5
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $6
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $7
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $10, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $12
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        add %rax, $2
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc %rdx, $3
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $4
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $5
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $6
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $7
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $10, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $13
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        add %rax, $3
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc %rdx, $4
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $5
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $6
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $7
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $10, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $14
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        add %rax, $4
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc %rdx, $5
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $6
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $7
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $10, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $15
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        add %rax, $5
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc %rdx, $6
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $7
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $11, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $12
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        add %rax, $3
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc %rdx, $4
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $5
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $6
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $7
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $11, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $13
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        add %rax, $4
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc %rdx, $5
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $6
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $7
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $11, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $14
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        add %rax, $5
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc %rdx, $6
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc $$0, $7
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mov $11, %rax
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mulq $15
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        add %rax, $6
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        adc %rdx, $7
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        &amp;#34;&lt;/span&gt;
      : &lt;span style=&#34;color:#75715e&#34;&gt;/* $0 */&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;={r8}&amp;#34;&lt;/span&gt;(result[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]),  &lt;span style=&#34;color:#75715e&#34;&gt;/* $1 */&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;={r9}&amp;#34;&lt;/span&gt;(result[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]),  &lt;span style=&#34;color:#75715e&#34;&gt;/* $2 */&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;={r10}&amp;#34;&lt;/span&gt;(result[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]),
        &lt;span style=&#34;color:#75715e&#34;&gt;/* $3 */&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;={r11}&amp;#34;&lt;/span&gt;(result[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]), &lt;span style=&#34;color:#75715e&#34;&gt;/* $4 */&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;={r12}&amp;#34;&lt;/span&gt;(result[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]), &lt;span style=&#34;color:#75715e&#34;&gt;/* $5 */&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;={r13}&amp;#34;&lt;/span&gt;(result[&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]),
        &lt;span style=&#34;color:#75715e&#34;&gt;/* $6 */&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;={r14}&amp;#34;&lt;/span&gt;(result[&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]), &lt;span style=&#34;color:#75715e&#34;&gt;/* $7 */&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;={r15}&amp;#34;&lt;/span&gt;(result[&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;])

      : &lt;span style=&#34;color:#75715e&#34;&gt;/* $8 */&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;m&amp;#34;&lt;/span&gt;(self_t[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]),   &lt;span style=&#34;color:#75715e&#34;&gt;/* $9 */&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;m&amp;#34;&lt;/span&gt;(self_t[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]),   &lt;span style=&#34;color:#75715e&#34;&gt;/* $10 */&lt;/span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;m&amp;#34;&lt;/span&gt;(self_t[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]),
        &lt;span style=&#34;color:#75715e&#34;&gt;/* $11 */&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;m&amp;#34;&lt;/span&gt;(self_t[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]),  &lt;span style=&#34;color:#75715e&#34;&gt;/* $12 */&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;m&amp;#34;&lt;/span&gt;(other_t[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]), &lt;span style=&#34;color:#75715e&#34;&gt;/* $13 */&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;m&amp;#34;&lt;/span&gt;(other_t[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]),
        &lt;span style=&#34;color:#75715e&#34;&gt;/* $14 */&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;m&amp;#34;&lt;/span&gt;(other_t[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]), &lt;span style=&#34;color:#75715e&#34;&gt;/* $15 */&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;m&amp;#34;&lt;/span&gt;(other_t[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;])
      : &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rax&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rdx&amp;#34;&lt;/span&gt;
      :
      );
    }

    U512(result)
  }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And here&amp;rsquo;s what that generates. I&amp;rsquo;ve heavily commented it so you can understand what&amp;rsquo;s going on even if you&amp;rsquo;ve never touched assembly in your life, but you will need to know basic low-level details like the difference between memory and registers. If you want to get a primer on the structure of a CPU, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_processing_unit#Structure_and_implementation&#34;&gt;Wikipedia article on structure and implementation of CPUs&lt;/a&gt; is a good place to start:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-assembly&#34; data-lang=&#34;assembly&#34;&gt;libbigint.so`disassemble:
    ;; Function prelude - this is generated by Rust
    pushq  %r15
    pushq  %r14
    pushq  %r13
    pushq  %r12
    subq   $0x40, %rsp

    ;; Load the input arrays into registers...
    movq   0x68(%rsp), %rax
    movq   0x70(%rsp), %rcx
    movq   0x78(%rsp), %rdx
    movq   0x80(%rsp), %rsi
    movq   0x88(%rsp), %r8
    movq   0x90(%rsp), %r9
    movq   0x98(%rsp), %r10
    movq   0xa0(%rsp), %r11

    ;; ...and then immediately back into memory
    ;; This is done by the Rust compiler. There is a way to avoid
    ;; this happening but I&amp;#39;ll get to that later
    ;; These four are the first input array
    movq   %rax, 0x38(%rsp)
    movq   %rcx, 0x30(%rsp)
    movq   %rdx, 0x28(%rsp)
    movq   %rsi, 0x20(%rsp)
    ;; These four are the output array, which is initialised to be
    ;; the same as the second input array.
    movq   %r8,  0x18(%rsp)
    movq   %r9,  0x10(%rsp)
    movq   %r10, 0x8(%rsp)
    movq   %r11, (%rsp)

    ;; This is the main loop, you&amp;#39;ll see the same code repeated many
    ;; times since it&amp;#39;s been unrolled so I won&amp;#39;t go over it every time.
    ;; This takes the form of a loop that looks like:
    ;;
    ;; for i in 0..U256_SIZE {
    ;;     for j in 0..U256_SIZE {
    ;;         /* Loop body */
    ;;     }
    ;; }

    ;; Load the `0`th element of the input array into the &amp;#34;%rax&amp;#34;
    ;; register so we can operate on it. The first element is actually
    ;; already in `%rax` at this point but it gets loaded again anyway.
    ;; This is because the `asm!` macro is hiding a lot of details, which
    ;; I&amp;#39;ll get to later.
    movq   0x38(%rsp), %rax
    ;; Multiply it with the `0`th element of the output array This operates
    ;; on memory rather than a register, and so is significantly slower than
    ;; if the same operation had been done on a register. Again, I&amp;#39;ll get to
    ;; that soon.
    mulq   0x18(%rsp)
    ;; `mulq` multiplies two 64-bit numbers and stores the low and high
    ;; 64 bits of the result in `%rax` and `%rdx`, respectively. We move
    ;; the low bits into `%r8` (the lowest 64 bits of the 512-bit result)
    ;; and the high bits into `%r9` (the second-lowest 64 bits of the
    ;; result).
    movq   %rax, %r8
    movq   %rdx, %r9

    ;; We do the same for `i = 0, j = 1`
    movq   0x38(%rsp), %rax
    mulq   0x10(%rsp)

    ;; Whereas above we moved the values into the output registers, this time
    ;; we have to add the results to the output.
    addq   %rax, %r9

    ;; Here we add 0 because the CPU will use the &amp;#34;carry bit&amp;#34; (whether or not
    ;; the previous addition overflowed) as an additional input. This is
    ;; essentially the same as adding 1 to `rdx` if the previous addition
    ;; overflowed.
    adcq   $0x0, %rdx

    ;; Then we move the upper 64 bits of the multiplication (plus the carry bit
    ;; from the addition) into the third-lowest 64 bits of the output.
    movq   %rdx, %r10

    ;; Then we continue for `j = 2` and `j = 3`
    movq   0x38(%rsp), %rax
    mulq   0x8(%rsp)
    addq   %rax,       %r10
    adcq   $0x0,       %rdx
    movq   %rdx,       %r11
    movq   0x38(%rsp), %rax
    mulq   (%rsp)
    addq   %rax,       %r11
    adcq   $0x0,       %rdx
    movq   %rdx,       %r12

    ;; Then we do the same for `i = 1`, `i = 2` and `i = 3`
    movq   0x30(%rsp), %rax
    mulq   0x18(%rsp)  
    addq   %rax,       %r9
    adcq   %rdx,       %r10
    adcq   $0x0,       %r11
    adcq   $0x0,       %r12

    ;; This `xor` just ensures that `%r13` is zeroed. Again, this is
    ;; non-optimal (we don&amp;#39;t need to zero these registers at all) but
    ;; I&amp;#39;ll get to that.
    xorq   %r13,       %r13
    adcq   $0x0,       %r13
    xorq   %r14,       %r14
    adcq   $0x0,       %r14
    xorq   %r15,       %r15
    adcq   $0x0,       %r15
    movq   0x30(%rsp), %rax
    mulq   0x10(%rsp)  
    addq   %rax,       %r10
    adcq   %rdx,       %r11
    adcq   $0x0,       %r12
    adcq   $0x0,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x30(%rsp), %rax
    mulq   0x8(%rsp)   
    addq   %rax,       %r11
    adcq   %rdx,       %r12
    adcq   $0x0,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x30(%rsp), %rax
    mulq   (%rsp)      
    addq   %rax,       %r12
    adcq   %rdx,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x28(%rsp), %rax
    mulq   0x18(%rsp)  
    addq   %rax,       %r10
    adcq   %rdx,       %r11
    adcq   $0x0,       %r12
    adcq   $0x0,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x28(%rsp), %rax
    mulq   0x10(%rsp)  
    addq   %rax,       %r11
    adcq   %rdx,       %r12
    adcq   $0x0,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x28(%rsp), %rax
    mulq   0x8(%rsp)   
    addq   %rax,       %r12
    adcq   %rdx,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x28(%rsp), %rax
    mulq   (%rsp)      
    addq   %rax,       %r13
    adcq   %rdx,       %r14
    adcq   $0x0,       %r15
    movq   0x20(%rsp), %rax
    mulq   0x18(%rsp)  
    addq   %rax,       %r11
    adcq   %rdx,       %r12
    adcq   $0x0,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x20(%rsp), %rax
    mulq   0x10(%rsp)  
    addq   %rax,       %r12
    adcq   %rdx,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x20(%rsp), %rax
    mulq   0x8(%rsp)   
    addq   %rax,       %r13
    adcq   %rdx,       %r14
    adcq   $0x0,       %r15
    movq   0x20(%rsp), %rax
    mulq   (%rsp)      
    addq   %rax,       %r14
    adcq   %rdx,       %r15

    ;; Finally, we move everything out of registers so we can
    ;; return it on the stack
    movq   %r8,   (%rdi)
    movq   %r9,   0x8(%rdi)
    movq   %r10,  0x10(%rdi)
    movq   %r11,  0x18(%rdi)
    movq   %r12,  0x20(%rdi)
    movq   %r13,  0x28(%rdi)
    movq   %r14,  0x30(%rdi)
    movq   %r15,  0x38(%rdi)
    movq   %rdi,  %rax
    addq   $0x40, %rsp
    popq   %r12
    popq   %r13
    popq   %r14
    popq   %r15
    retq   &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So as you can see from my comments, there are a lot of inefficiencies in this code. We multiply on variables from memory instead of from registers, we do superfluous stores and loads, also the CPU has to do many stores and loads before even getting to the &amp;ldquo;real&amp;rdquo; code (the multiply-add loop), which is important because although the CPU can do loads and stores in parallel with calculations, the way that this code is written requires it to wait for everything to be loaded before it starts doing calculations. This is because the &lt;code&gt;asm&lt;/code&gt; macro hides a lot of details. Essentially you&amp;rsquo;re telling the compiler to put the input data wherever it likes, and then to substitute wherever it put the data into your assembly code with string manipulation. The compiler stores everything into registers, but then we instruct it to put the input arrays in memory (with the &lt;code&gt;&amp;quot;m&amp;quot;&lt;/code&gt; before the input parameters) so it loads it back into memory again. There are ways that you could write this code to remove the inefficiencies in it, but it is clearly very difficult for even a seasoned professional to write the correct code here. This code is bug-prone - if you hadn&amp;rsquo;t zeroed the output registers with the series of &lt;code&gt;xor&lt;/code&gt; instructions then the code would fail sometimes but not always, with seemingly-random values that depended on the calling function&amp;rsquo;s internal state. It could probably be sped up by replacing &lt;code&gt;&amp;quot;m&amp;quot;&lt;/code&gt; with &lt;code&gt;&amp;quot;r&amp;quot;&lt;/code&gt; here (I hadn&amp;rsquo;t tested that because I only realised that this is a problem while investigating why the old assembly was so much slower in the course of writing this article), but that&amp;rsquo;s not clear from reading the source code of the program and only someone with quite in-depth knowledge of LLVM&amp;rsquo;s assembly syntax would realise that when looking at the code.&lt;/p&gt;

&lt;p&gt;By comparison, the Rust code that uses &lt;code&gt;u128&lt;/code&gt; is about as say-what-you-mean as you can get. Even if your goal was not optimisation you would probably write something similar to it as the simplest solution to the problem, but the code that LLVM produces is very high-quality. You can see already that it&amp;rsquo;s not too different to our hand-written code, but it addresses some of the issues (commented below) while also including a couple more optimisations that I wouldn&amp;rsquo;t have even thought of. I couldn&amp;rsquo;t find any significant optimisations that it missed.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the generated assembly:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-assembly&#34; data-lang=&#34;assembly&#34;&gt;libbigint.so`disassemble:
    ;; Function prelude
    pushq  %rbp
    movq   %rsp, %rbp
    pushq  %r15
    pushq  %r14
    pushq  %r13
    pushq  %r12
    pushq  %rbx
    subq   $0x48, %rsp

    movq   0x10(%rbp), %r11
    movq   0x18(%rbp), %rsi

    movq   %rsi, -0x38(%rbp)

    ;; I originally thought that this was a missed optimisation,
    ;; but it actually has to do this (instead of doing
    ;; `movq 0x30(%rbp), %rax`) because the `%rax` register gets
    ;; clobbered by the `mulq` below. This means it can multiply
    ;; the first element of the first array by each of the
    ;; elements of th without having to reload it from memory
    ;; like the hand-written assembly does.
    movq   0x30(%rbp), %rcx
    movq   %rcx,       %rax

    ;; LLVM multiplies from a register instead of from memory
    mulq   %r11

    ;; LLVM moves `%rdx` (the upper bits) into a register, since
    ;; we need to operate on it further. It moves `%rax` (the
    ;; lower bits) directly into memory because we don&amp;#39;t need
    ;; to do any further work on it. This is better than moving
    ;; in and out of memory like we do in the previous code.
    movq   %rdx, %r9
    movq   %rax, -0x70(%rbp)
    movq   %rcx, %rax
    mulq   %rsi
    movq   %rax, %rbx
    movq   %rdx, %r8

    movq   0x20(%rbp), %rsi
    movq   %rcx,       %rax
    mulq   %rsi

    ;; LLVM uses `%r13` as an intermediate because it needs this
    ;; value in `%r13` later to operate on it anyway.
    movq   %rsi,       %r13
    movq   %r13,       -0x40(%rbp)

    ;; Again, we have to operate on both the low and high bits
    ;; so LLVM moves them both into registers.
    movq   %rax,       %r10
    movq   %rdx,       %r14
    movq   0x28(%rbp), %rdx
    movq   %rdx,       -0x48(%rbp)
    movq   %rcx,       %rax
    mulq   %rdx
    movq   %rax,       %r12
    movq   %rdx,       -0x58(%rbp)
    movq   0x38(%rbp), %r15
    movq   %r15,       %rax
    mulq   %r11
    addq   %r9,        %rbx
    adcq   %r8,        %r10

    ;; These two instructions store the flags into the `%rcx`
    ;; register.
    pushfq 
    popq   %rcx
    addq   %rax, %rbx
    movq   %rbx, -0x68(%rbp)
    adcq   %rdx, %r10

    ;; This stores the flags from the previous calculation into
    ;; `%r8`.
    pushfq 
    popq   %r8

    ;; LLVM takes the flags back out of `%rcx` and then does an
    ;; add including the carry flag. This is smart. It means we
    ;; don&amp;#39;t need to do the weird-looking addition of zero since
    ;; we combine the addition of the carry flag and the addition
    ;; of the number&amp;#39;s components together into one instruction.
    ;;
    ;; It&amp;#39;s possible that the way LLVM does it is faster on modern
    ;; processors, but storing this in `%rcx` is unnecessary,
    ;; because the flags would be at the top of the stack anyway
    ;; (i.e. you could remove the `popq %rcx` above and this
    ;; `pushq %rcx` and it would act the same). If it is slower
    ;; then the difference will be negligible.
    pushq  %rcx
    popfq  
    adcq   %r14, %r12

    pushfq 
    popq   %rax
    movq   %rax,        -0x50(%rbp)
    movq   %r15,        %rax
    movq   -0x38(%rbp), %rsi
    mulq   %rsi
    movq   %rdx,        %rbx
    movq   %rax,        %r9
    addq   %r10,        %r9
    adcq   $0x0,        %rbx
    pushq  %r8
    popfq  
    adcq   $0x0,        %rbx

    ;; `setb` is used instead of explicitly zeroing registers and
    ;; then adding the carry bit. `setb` just sets the byte at the
    ;; given address to 1 if the carry flag is set (since this is
    ;; basically a `mov` it&amp;#39;s faster than zeroing and then adding)
    setb   -0x29(%rbp)
    addq   %r12, %rbx

    setb   %r10b
    movq   %r15, %rax
    mulq   %r13
    movq   %rax, %r12
    movq   %rdx, %r8
    movq   0x40(%rbp), %r14
    movq   %r14, %rax
    mulq   %r11
    movq   %rdx, %r13
    movq   %rax, %rcx
    movq   %r14, %rax
    mulq   %rsi
    movq   %rdx, %rsi
    addq   %r9, %rcx
    movq   %rcx, -0x60(%rbp)

    ;; This is essentially a hack to add `%r12` and `%rbx` and store
    ;; the output in `%rcx`. It&amp;#39;s one instruction instead of the two
    ;; that would be otherwise required. `leaq` is the take-address-of
    ;; instruction, so this line is essentially the same as if you did
    ;; `&amp;amp;((void*)first)[second]` instead of `first + second` in C. In
    ;; assembly, though, there are no hacks. Every dirty trick is fair
    ;; game.
    leaq   (%r12,%rbx), %rcx

    ;; The rest of the code doesn&amp;#39;t have any new tricks, just the same
    ;; ones repeated.
    adcq   %rcx,        %r13
    pushfq 
    popq   %rcx
    addq   %rax,        %r13
    adcq   $0x0,        %rsi
    pushq  %rcx
    popfq  
    adcq   $0x0,        %rsi
    setb   -0x2a(%rbp)
    orb    -0x29(%rbp), %r10b
    addq   %r12,        %rbx
    movzbl %r10b,       %ebx
    adcq   %r8,         %rbx
    setb   %al
    movq   -0x50(%rbp), %rcx
    pushq  %rcx
    popfq  
    adcq   -0x58(%rbp), %rbx
    setb   %r8b
    orb    %al,         %r8b
    movq   %r15,        %rax
    mulq   -0x48(%rbp)
    movq   %rdx,        %r12
    movq   %rax,        %rcx
    addq   %rbx,        %rcx
    movzbl %r8b,        %eax
    adcq   %rax,        %r12
    addq   %rsi,        %rcx
    setb   %r10b
    movq   %r14,        %rax
    mulq   -0x40(%rbp)
    movq   %rax,        %r8
    movq   %rdx,        %rsi
    movq   0x48(%rbp),  %r15
    movq   %r15,        %rax
    mulq   %r11
    movq   %rdx,        %r9
    movq   %rax,        %r11
    movq   %r15,        %rax
    mulq   -0x38(%rbp)
    movq   %rdx,        %rbx
    addq   %r13,        %r11
    leaq   (%r8,%rcx),  %rdx
    adcq   %rdx,        %r9
    pushfq 
    popq   %rdx
    addq   %rax,        %r9
    adcq   $0x0,        %rbx
    pushq  %rdx
    popfq  
    adcq   $0x0,        %rbx
    setb   %r13b
    orb    -0x2a(%rbp), %r10b
    addq   %r8,         %rcx
    movzbl %r10b,       %ecx
    adcq   %rsi,        %rcx
    setb   %al
    addq   %r12,        %rcx
    setb   %r8b
    orb    %al,         %r8b
    movq   %r14,        %rax
    movq   -0x48(%rbp), %r14
    mulq   %r14
    movq   %rdx,        %r10
    movq   %rax,        %rsi
    addq   %rcx,        %rsi
    movzbl %r8b,        %eax
    adcq   %rax,        %r10
    addq   %rbx,        %rsi
    setb   %cl
    orb    %r13b,       %cl
    movq   %r15,        %rax
    mulq   -0x40(%rbp)
    movq   %rdx,        %rbx
    movq   %rax,        %r8
    addq   %rsi,        %r8
    movzbl %cl,         %eax
    adcq   %rax,        %rbx
    setb   %al
    addq   %r10,        %rbx
    setb   %cl
    orb    %al,         %cl
    movq   %r15,        %rax
    mulq   %r14
    addq   %rbx,        %rax
    movzbl %cl,         %ecx
    adcq   %rcx,        %rdx
    movq   -0x70(%rbp), %rcx
    movq   %rcx,        (%rdi)
    movq   -0x68(%rbp), %rcx
    movq   %rcx,        0x8(%rdi)
    movq   -0x60(%rbp), %rcx
    movq   %rcx,        0x10(%rdi)
    movq   %r11,        0x18(%rdi)
    movq   %r9,         0x20(%rdi)
    movq   %r8,         0x28(%rdi)
    movq   %rax,        0x30(%rdi)
    movq   %rdx,        0x38(%rdi)
    movq   %rdi,        %rax
    addq   $0x48,       %rsp
    popq   %rbx
    popq   %r12
    popq   %r13
    popq   %r14
    popq   %r15
    popq   %rbp
    retq   &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Although there are a few more instructions in the LLVM-generated version, the slowest type of instruction (loads and stores) are minimised, it (for the most part) avoids redundant work and it applies many cheeky optimisations on top. The end result is that the code runs significantly faster.&lt;/p&gt;

&lt;p&gt;This is not the first time that a carefully-written Rust implementation has outperformed our assembly code - some months ago I rewrote the Rust implementations of addition and subtraction, making them outperform the assembly implementation by 20% and 15%, respectively. Those didn&amp;rsquo;t require 128-bit arithmetic to beat the assembly (to get the full power of the hardware in Rust you only need &lt;code&gt;u64::checked_add&lt;/code&gt;/&lt;code&gt;checked_sub&lt;/code&gt;), although who knows - maybe in a future PR we&amp;rsquo;ll use 128-bit arithmetic and see the speed improve further still.&lt;/p&gt;

&lt;p&gt;You can see the code from this PR &lt;a href=&#34;https://github.com/paritytech/bigint/pull/38&#34;&gt;here&lt;/a&gt; and the code from the addition/subtraction PR &lt;a href=&#34;https://github.com/paritytech/bigint/pull/26&#34;&gt;here&lt;/a&gt;. I should note that although the latter PR shows multiplication already outperforming the assembly implementation, this was actually due to a benchmark that mostly multiplied numbers with 0. Whoops. If there&amp;rsquo;s something we can learn from that, it&amp;rsquo;s that there can be no informed optimisation without representative benchmarks.&lt;/p&gt;

&lt;p&gt;My point is not that we should take what we&amp;rsquo;ve learnt from the LLVM-generated code and write a new version of our hand-rolled assembly. The point is that optimising compilers are &lt;em&gt;really good&lt;/em&gt;. There are very smart people working on them and computers are really good at this kind of optimisation problem (in the mathematic sense) in a way that humans find quite difficult. It&amp;rsquo;s the job of language designers to give us the tools we need to inform the optimiser as best we can as to what our true intent is, and larger integer sizes are another step towards that. Rust has done a great job of allowing programmers to write programs that are easily understandable by humans and compilers alike, and it&amp;rsquo;s just that power that has largely driven its success.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Acheiving warp speed with Rust</title>
      <link>http://troubles.md/posts/rust-optimization/</link>
      <pubDate>Wed, 17 May 2017 12:00:33 +0200</pubDate>
      
      <guid>http://troubles.md/posts/rust-optimization/</guid>
      <description>

&lt;h3 id=&#34;contents&#34;&gt;Contents:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#number-one-optimization-tip-dont&#34;&gt;Number one optimization tip: don&amp;rsquo;t&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#never-optimize-blindly&#34;&gt;Never optimize blindly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dont-bother-optimizing-one-time-costs&#34;&gt;Don&amp;rsquo;t bother optimizing one-time costs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#improve-your-algorithms&#34;&gt;Improve your algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cpu-architecture-primer&#34;&gt;CPU architecture primer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#keep-as-much-as-possible-in-cache&#34;&gt;Keep as much as possible in cache&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#keep-as-much-as-possible-in-registers&#34;&gt;Keep as much as possible in registers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#avoid-boxtrait&#34;&gt;Avoid &lt;code&gt;Box&amp;lt;Trait&amp;gt;&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#use-stack-based-variable-length-datatypes&#34;&gt;Use stack-based variable-length datatypes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loop-unrolling-is-still-cool&#34;&gt;Loop unrolling is still cool&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#assert-conditions-beforehand&#34;&gt;&lt;code&gt;assert!&lt;/code&gt; conditions beforehand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#use-link-time-optimization&#34;&gt;Use link-time optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dont-use-inlinealways&#34;&gt;Don&amp;rsquo;t use &lt;code&gt;#[inline(always)]&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parallelize-but-not-how-you-think&#34;&gt;Parallelize, but not how you think&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-case-study&#34;&gt;A case study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wrapping-up&#34;&gt;Wrapping up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;If you&amp;rsquo;re looking to write fast code in Rust, good news! Rust makes it really
easy to write really fast code. The focus on zero-cost abstractions, the
lack of implicit boxing and the static memory management means that even nave
code is often faster than the equivalent in other languages, and certainly
faster than nave code in any equally-safe language. Maybe, though, like most
programmers you&amp;rsquo;ve spent your whole programming career safely insulated from
having to think about any of the details of the machine, and now you want to dig
a little deeper and find out the real reason that Python script you rewrote in
Rust runs 100x faster and uses a 10th of the memory. After all, they both do the
same thing and run on the same CPU, right?&lt;/p&gt;

&lt;p&gt;So, here&amp;rsquo;s an optimization guide, aimed at those who know how to program but
maybe don&amp;rsquo;t know how it maps to real ones and zeroes on the bare metal of your
CPU. I&amp;rsquo;ll try to weave practical tips about optimizing Rust code with
explanations of the reason why it&amp;rsquo;s faster than the alternative, and we&amp;rsquo;ll end
with a case study from the Rust standard library.&lt;/p&gt;

&lt;p&gt;This post assumes decent familiarity with programming, a beginner&amp;rsquo;s familiarity
with Rust and almost no familiarity with CPU architecture.&lt;/p&gt;

&lt;h2 id=&#34;number-one-optimization-tip-don-t&#34;&gt;Number one optimization tip: don&amp;rsquo;t&lt;/h2&gt;

&lt;p&gt;Ok, I&amp;rsquo;ll start with a few disclaimers before I get into the meat. Firstly,
unless you&amp;rsquo;re running into performance problems in real-life usage, optimize
your code for readability before you optimize it for runtime performance.
Compilers and humans alike are better at understanding boring, straightforward
code, so if you write that first you&amp;rsquo;ll be likely to have &amp;ldquo;good enough&amp;rdquo;
performance with the added benefit of being maintainable. If you write nice,
refactorable code it&amp;rsquo;ll be easy to change if you later realize that it&amp;rsquo;s being
wasteful.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s not to say that performance doesn&amp;rsquo;t matter. You shouldn&amp;rsquo;t only optimize
&lt;em&gt;slow&lt;/em&gt; programs. A long-running program using high amounts of CPU but not
causing visible slowness is just as bad as a program that takes 30s instead of
3s to process some data, it&amp;rsquo;s just that the former is wasting battery and the
latter is wasting time. Weigh up the time it would take to optimize the code
against the benefit you would get from the optimization.&lt;/p&gt;

&lt;p&gt;The reason maintainability is so important is that lot of optimizations are
&amp;ldquo;try it and see&amp;rdquo; - if you&amp;rsquo;re unable to make sweeping changes to your code
without breaking it you&amp;rsquo;re going to have a really bad time. Now, speaking of
&amp;ldquo;try it and see&amp;rdquo;&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;never-optimize-blindly&#34;&gt;Never optimize blindly&lt;/h2&gt;

&lt;p&gt;There are lots of performance-tracing tools out there. A famous tool/set of
tools for C and other systems languages is &lt;a href=&#34;http://valgrind.org/&#34;&gt;&lt;code&gt;valgrind&lt;/code&gt;&lt;/a&gt;, which are
extremely powerful but can be scary to get started with, so if you just want to
have a quick overview of what your program is doing from a performance
standpoint check out &lt;a href=&#34;http://blog.adamperry.me/rust/2016/07/24/profiling-rust-perf-flamegraph/&#34;&gt;this article on analyzing Rust with &lt;code&gt;perf&lt;/code&gt;&lt;/a&gt;, a
fantastic and easy-to-use performance tool for Linux. Unless there&amp;rsquo;s a glaring
flaw, like pervasive &lt;code&gt;clone&lt;/code&gt;ing or a blatantly sub-optimal algorithm, &lt;code&gt;perf&lt;/code&gt;
will likely give better results than simply optimizing stuff that &amp;ldquo;looks slow&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Another tool that&amp;rsquo;s good to help you avoid gotchas of all kinds (not just
performance) is &lt;a href=&#34;https://github.com/Manishearth/rust-clippy&#34;&gt;&lt;code&gt;clippy&lt;/code&gt;&lt;/a&gt;, but you already knew that because you&amp;rsquo;re
using it on all your code to begin with, right?&lt;/p&gt;

&lt;p&gt;&lt;code&gt;perf&lt;/code&gt; also shows the cumulative cost of each function over the course of the
program&amp;rsquo;s runtime, which leads me to my next point:&lt;/p&gt;

&lt;h2 id=&#34;don-t-bother-optimizing-one-time-costs&#34;&gt;Don&amp;rsquo;t bother optimizing one-time costs&lt;/h2&gt;

&lt;p&gt;Your config-parsing code can be as slow as you like and it&amp;rsquo;s unlikely to matter.
Don&amp;rsquo;t just optimize the slowest function in your program, optimize the one that
takes up the most of your runtime. Those may be the same function, but if you
get a 2 millisecond improvement on a function that&amp;rsquo;s called 1000 times, that&amp;rsquo;s
better than a 1 second improvement on a function that&amp;rsquo;s called once.&lt;/p&gt;

&lt;h2 id=&#34;improve-your-algorithms&#34;&gt;Improve your algorithms&lt;/h2&gt;

&lt;p&gt;Now, as with every article on performance, this is where I add in the requisite
disclaimer of &lt;em&gt;use better algorithms first&lt;/em&gt;. Don&amp;rsquo;t invent new algorithms unless
you do that for a living, but in all likelihood if you&amp;rsquo;re running into
performance problems it&amp;rsquo;s more likely to be due to poor algorithms than to poor
implementations. Most programmers test their code on small datasets, but if you
have &lt;code&gt;O(n)&lt;/code&gt; complexity that won&amp;rsquo;t appear until you&amp;rsquo;ve tried it on a larger
dataset. If you don&amp;rsquo;t know what your algorithm is, which is likely since most
code is written without a specific algorithm in mind, just try to have as few
loops as possible and remember that every use of &lt;code&gt;collect&lt;/code&gt; has to iterate over
the entire collection at least once, and so the more work you can do using less
loops the better. This is the same as optimization in any language though, so
this is all I&amp;rsquo;ll say on algorithmic complexity for now. If you want to find out
more, there are some excellent resources out there.&lt;/p&gt;

&lt;h2 id=&#34;cpu-architecture-primer&#34;&gt;CPU architecture primer&lt;/h2&gt;

&lt;p&gt;So maybe the complexity of your algorithm can&amp;rsquo;t be improved, and to make any
improvements you need to get into the down-to-the-metal stuff. This is where the
difference comes in between languages like Rust and C and languages like Python
and Ruby. It&amp;rsquo;s entirely possible that you know this already, but it&amp;rsquo;s worth
going over to make sure we&amp;rsquo;re all on the same page.&lt;/p&gt;

&lt;p&gt;There are two parts to any computation, the stuff it does and the stuff it does
it on. The instructions, and the data.&lt;/p&gt;

&lt;p&gt;Instructions are stored in the instruction cache - a chunk of really, really
fast memory that&amp;rsquo;s directly readable by the CPU. Each instruction can put and/or
take data from the CPU&amp;rsquo;s registers, which is a small number of small pieces of
memory, either 32 or 64 bits depending on your computer&amp;rsquo;s word size. Only a
small amount of data can be in registers at any one time, however, and you can&amp;rsquo;t
take a pointer to a register, so sometimes the CPU must access the computer&amp;rsquo;s
RAM. Since RAM is slow, the CPU tries to read in bulk and then store the result
in increasingly small, increasingly fast caches. If it tries to access data that
isn&amp;rsquo;t in the smallest cache, it has to read the slightly larger cache,
continuing up until it reaches RAM. The upshot is: you want to keep your data as
small as possible, and for data that is accessed together to be close to each
other so the CPU loads as much of it at once as possible. This should be enough
information to get you through the rest of this article, but if you want to dive
deeper into it you can check out the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_processing_unit#Structure_and_implementation&#34;&gt;structure and implementation section in
the Wikipedia page for the CPU&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;keep-as-much-as-possible-in-cache&#34;&gt;Keep as much as possible in cache&lt;/h2&gt;

&lt;p&gt;The further away your data is from your CPU the slower your program will be. The
very worst place for data to be is on a different computer. A less awful - but
still very awful - place for data to be is on your hard drive. Better still is
in your RAM but as mentioned before, RAM is slow. &lt;em&gt;Almost&lt;/em&gt; the best possible
place for your data is in CPU cache. You may have heard some folklore that
allocating is bad, and this is the main reason why. Accessing two different
locations one after another on the stack is fine, since they&amp;rsquo;re likely to be on
the same cache line. Accessing two different locations one after another on the
heap is significantly slower, since it&amp;rsquo;s much less likely they they&amp;rsquo;re directly
next to each other. We&amp;rsquo;ll go into exactly why this is in a moment.&lt;/p&gt;

&lt;p&gt;If you have to allocate, because you need variable-size containers, shared
ownership or owned trait objects (see below for why you probably don&amp;rsquo;t need
trait objects), try to put data that will be accessed in
sequence in order in RAM, so that when the CPU reads one element it necessarily
has to read the next few elements too, meaning it doesn&amp;rsquo;t need to stall waiting
for RAM in order to operate on them.&lt;/p&gt;

&lt;p&gt;As a rule of thumb for whether something has to allocate: if you can tell me the
amount of space the value will use up without running the program, it&amp;rsquo;s stored
on the stack. If you don&amp;rsquo;t know something&amp;rsquo;s size until runtime, it&amp;rsquo;s allocated
on the heap.&lt;/p&gt;

&lt;p&gt;This means that &lt;code&gt;String&lt;/code&gt;, &lt;code&gt;Vec&lt;/code&gt;, &lt;code&gt;HashMap&lt;/code&gt; and &lt;code&gt;Box&amp;lt;Trait&amp;gt;&lt;/code&gt;/&lt;code&gt;Box&amp;lt;[T]&amp;gt;&lt;/code&gt; all
allocate, but any user-defined struct does not (it may contain something that
&lt;em&gt;does&lt;/em&gt; allocate, but it doesn&amp;rsquo;t require any extra allocation to construct if
you already have an instance of the allocating type). &lt;code&gt;Box&amp;lt;T&amp;gt;&lt;/code&gt; where &lt;code&gt;T&lt;/code&gt; has a
statically-known size also allocates, so be careful of recursive enums. If
you&amp;rsquo;re creating a tree structure that then becomes immutable, like with an AST,
you might want to consider using a &lt;code&gt;TypedArena&lt;/code&gt; to get tighter control over
memory use. &lt;code&gt;TypedArena&lt;/code&gt; is still unstable though, and it increases complexity,
so it&amp;rsquo;s not suitable for all use-cases.&lt;/p&gt;

&lt;p&gt;This is why you may have heard some complaints about Haskell&amp;rsquo;s use of a linked
list of characters to represent a string. I&amp;rsquo;m not going to beat &lt;a href=&#34;http://cglab.ca/~abeinges/blah/too-many-lists/book/#an-obligatory-public-service-announcement&#34;&gt;gankro&amp;rsquo;s
wonderful rant on the subject&lt;/a&gt;, but suffice to say that this is
more-or-less the worst possible data structure to choose when each individual
element is small and the number of elements is large, because it needs to store
a pointer for every element, instead of a single integer for the entire array.&lt;/p&gt;

&lt;p&gt;Not only that, but without some truly crazy compiler optimizations this means
that each element may not be directly after the element before it in memory.
We&amp;rsquo;ll get to how to calculate this in a moment, but essentially that means that
Haskell&amp;rsquo;s &lt;code&gt;String&lt;/code&gt; type can cause a cache miss up to twice &lt;em&gt;per element&lt;/em&gt;,
whereas if you had a vector of &lt;code&gt;Char&lt;/code&gt;s (assuming 32-bit chars) it could only
cause a maximum of 1 cache miss for every 16 elements. This is why the
performance-savvy in languages such as Haskell and Lisp know to use vector-like
constructs when possible.&lt;/p&gt;

&lt;p&gt;Back in the world of Rust, this means that you should avoid indirection-heavy
representations &lt;code&gt;Vec&amp;lt;Vec&amp;lt;_&amp;gt;&amp;gt;&lt;/code&gt; to represent a matrix, since this means that each
sub-vec will likely be in a different location. Use a data structure that uses a
flat &lt;code&gt;Vec&lt;/code&gt; as a backing array and builds on top of it, unless you really do need
to have the inner &lt;code&gt;Vec&lt;/code&gt;s be both jagged (each sub-vec is of different size) and
growable (you can change the size of each sub-vec independently at runtime). You
probably don&amp;rsquo;t need either, let alone both. If you need them to be a uniform
size, store a &lt;code&gt;Vec&lt;/code&gt; and a dimension tuple, if you need them to be jagged but
don&amp;rsquo;t need them to be growable, store a list of indices and return slices of the
flat &lt;code&gt;Vec&lt;/code&gt; using those. For an example of why this is good, let&amp;rsquo;s dive into some
basic math. Let&amp;rsquo;s assume a matrix backed by flat vector and a number of columns
(the number of rows can be inferred from columns + data length):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// This isn&amp;#39;t how Vec is defined in the standard library but it&amp;#39;s a simplified
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// version with the same memory layout.
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; Vec&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;T&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; {
    pointer: &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; T,
    capacity: &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;,
    len: &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;,
}

&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Matrix&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;T&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; {
    data: Vec&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;T&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;,
    num_columns: &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;,
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So a matrix with N rows and M columns needs &lt;code&gt;N * M * size_of::&amp;lt;T&amp;gt;()&lt;/code&gt; space for
the elements, plus &lt;code&gt;size_of::&amp;lt;*mut T&amp;gt;() + 3 * size_of::&amp;lt;usize&amp;gt;()&lt;/code&gt; for the
&amp;ldquo;metadata&amp;rdquo; (the vector&amp;rsquo;s pointer and the &lt;code&gt;capacity&lt;/code&gt;, &lt;code&gt;length&lt;/code&gt; and &lt;code&gt;num_columns&lt;/code&gt;
fields). If we&amp;rsquo;re on a 64-bit CPU with 64 byte cache lines like the i7, we end
up with both &lt;code&gt;*mut T&lt;/code&gt; and &lt;code&gt;usize&lt;/code&gt; being 4 bytes each. If we had a 4x4 matrix of
&lt;code&gt;f32&lt;/code&gt; (also 4 bytes in size) this would mean:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;Metadata size = 4 + 3 * 4 = 16
Maximum 2 cache misses

Data size = 4 * 4 * 4 = 64
Maximum 2 cache misses&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since the metadata and the data could be in separate parts of memory, we have to
calculate maximum cache misses separately. Both the metadata and the data could
cross a cache line and require two cache misses to load. This means that the
whole matrix would miss the cache 4 times in the worst case. If we had a
&lt;code&gt;Vec&amp;lt;Vec&amp;lt;f32&amp;gt;&amp;gt;&lt;/code&gt; representation that would mean the size of:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;Matrix metadata size = 4 + 2 * 4 = 12
Maximum 2 cache misses

Inner vector metadata size = 4 * (4 + 2 * 4) = 48
Maximum 2 cache misses

Data size = 4 * 4 = 16
Maximum 2 cache misses per row (8 cache misses total)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This means that the &lt;code&gt;Vec&amp;lt;Vec&amp;lt;f32&amp;gt;&amp;gt;&lt;/code&gt; representation could miss the cache up to
12 times to read the whole array, much worse than the flat representation.&lt;/p&gt;

&lt;p&gt;Even better, if you statically know the matrix&amp;rsquo;s size you can use
statically-sized arrays like &lt;code&gt;[[T; N]; N]&lt;/code&gt;. These are even cheaper than flat
vectors, although you obviously can&amp;rsquo;t use them for data with a variable size at
runtime. The 4x4 array in the previous example would be &lt;code&gt;[[f32; 4]; 4]&lt;/code&gt; and take
up 64 bytes, meaning that it would only take 2 cache misses to load in the worst
case.&lt;/p&gt;

&lt;h2 id=&#34;keep-as-much-as-possible-in-registers&#34;&gt;Keep as much as possible in registers&lt;/h2&gt;

&lt;p&gt;Now, the absolute best place for your data - registers. The more work you can do
without non-local writes the more that rustc and LLVM can assume about your
data&amp;rsquo;s access patterns. This is good because it means that data can be mapped to
the CPU&amp;rsquo;s physical registers, which are the fastest memory on your entire
computer, but even better, if you make your data suitable for registers then
anything that happens to that data can be aggressively optimized. Writes and
reads through pointers have certain ordering restrictions on them that prevent
optimization, but there are no such restrictions on register-allocated data.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s worth noting that since Rust restricts pointers more than C does, the
ordering restrictions on pointers could be relaxed. This hasn&amp;rsquo;t been implemented
in LLVM yet since most of the optimization work is based on leveraging the rules
of C and C-family languages. Even if they did implement relaxations on the
reordering rules, however, storing data in registers will still be easier to
optimize.&lt;/p&gt;

&lt;p&gt;So how do you get rustc to allocate things to registers? Essentially, the less
pointers you have to write at runtime the better. Writing to local variables
is better than writing through a mutable pointer. As much as possible, you
should try to constrain mutable writes to the data that you have ownership over.
So a mutable loop counter is fine, but passing a mutable reference to a loop
counter through multiple layers of functions is not (unless they end up getting
inlined, of course). This is really just an extension of one of my first points:
clean, boring code is easier to optimize than spaghetti.&lt;/p&gt;

&lt;h2 id=&#34;avoid-box-trait&#34;&gt;Avoid &lt;code&gt;Box&amp;lt;Trait&amp;gt;&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;The canonical way to create trait objects is &lt;code&gt;Box&amp;lt;Trait&amp;gt;&lt;/code&gt;, but the majority of
code can get away with &lt;code&gt;&amp;amp;mut Trait&lt;/code&gt;, which also has dynamic dispatch but saves
an allocation. If you absolutely need ownership then use a &lt;code&gt;Box&lt;/code&gt;, but most
use-cases can use an &lt;code&gt;&amp;amp;Trait&lt;/code&gt; or &lt;code&gt;&amp;amp;mut Trait&lt;/code&gt;. Even better is to avoid using a
trait object all together. &lt;code&gt;impl Trait&lt;/code&gt; is the obvious way to avoid them, but
that doesn&amp;rsquo;t allow you to store a heterogenous collection of elements that
implement a single trait since it&amp;rsquo;s basically type inference in a fancy hat. A
good trick for when you want to allow a variable but finite number of
implementors of a type because you want to choose between them or iterate over
them, use either a tuple or a recursive generic struct like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Cons&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Head, Tail&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(Head, Tail);&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since data structures in Rust don&amp;rsquo;t add any indirection or space overhead, you
can implement a trait for this structure recursively and have a function that
can take any number of parameters that runs as fast as an equivalent function
that takes a fixed number of parameters. Here&amp;rsquo;s an example of how this could
look for a function that takes a list of functions and calls them:&lt;/p&gt;

&lt;p&gt;Allocating version:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;call_all_fns&lt;/span&gt;(fns: Vec&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Box&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;FnBox() -&amp;gt; ()&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;) {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; f &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; fns {
        f();
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Allocation-free version:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Cons&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;First, Second&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(First, Second);

&lt;span style=&#34;color:#66d9ef&#34;&gt;trait&lt;/span&gt; HCons: Sized {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cons&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;T&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(self, other: &lt;span style=&#34;color:#a6e22e&#34;&gt;T&lt;/span&gt;) -&amp;gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Cons&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Self, T&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; {
        Cons(self, other)
    }
}

&lt;span style=&#34;color:#66d9ef&#34;&gt;impl&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;T: Sized&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; HCons &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; T {}

&lt;span style=&#34;color:#75715e&#34;&gt;// This is a hack to get around the fact that manually implementing the `Fn`
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// traits is currently unstable.
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;trait&lt;/span&gt; Callable {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;call&lt;/span&gt;(self);
}

&lt;span style=&#34;color:#66d9ef&#34;&gt;impl&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;F: Fn() -&amp;gt; ()&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; Callable &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; F {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;call&lt;/span&gt;(self) { self() }
}

&lt;span style=&#34;color:#66d9ef&#34;&gt;impl&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;First: &lt;span style=&#34;color:#a6e22e&#34;&gt;Callable&lt;/span&gt;, Second: &lt;span style=&#34;color:#a6e22e&#34;&gt;Callable&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; Callable &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; Cons&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;First, Second&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;call&lt;/span&gt;(self) {
        self.&lt;span style=&#34;color:#ae81ff&#34;&gt;0.&lt;/span&gt;call();
        self.&lt;span style=&#34;color:#ae81ff&#34;&gt;1.&lt;/span&gt;call();
    }
}

&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;call_all_fns_no_alloc&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;T: &lt;span style=&#34;color:#a6e22e&#34;&gt;Callable&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(fns: &lt;span style=&#34;color:#a6e22e&#34;&gt;T&lt;/span&gt;) {
    fns.call();
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here&amp;rsquo;s what they both look like in use:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; first_fn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; { println&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hello!&amp;#34;&lt;/span&gt;); };
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; second_fn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; { println&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;World!&amp;#34;&lt;/span&gt;); };
    
    call_all_fns(vec&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;[Box::new(first_fn), Box::new(second_fn)]);
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; first_fn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; { println&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hello!&amp;#34;&lt;/span&gt;); };
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; second_fn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; { println&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;World!&amp;#34;&lt;/span&gt;); };
    
    call_all_fns_no_alloc(first_fn.cons(second_fn));
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The functions passed to &lt;code&gt;call_all_fns_no_alloc&lt;/code&gt; are eligible for inlining, they
require no space overhead, and their instructions and data are directly next to
each other in memory and are therefore much faster to access than if each of
them were boxed. For example, in &lt;code&gt;combine&lt;/code&gt; there&amp;rsquo;s a &lt;code&gt;choice&lt;/code&gt; function that
takes an array that could contain trait objects, but it also supplies a &lt;code&gt;.or()&lt;/code&gt;
combinator (and a &lt;code&gt;choice!&lt;/code&gt; macro that expands to recursive &lt;code&gt;.or&lt;/code&gt; calls) that
returns an &lt;code&gt;Or&amp;lt;A, B&amp;gt;&lt;/code&gt; that in turn implements &lt;code&gt;Parser&lt;/code&gt;. This means that dispatch
is static and the objects are all stored in order in memory (because it&amp;rsquo;s just a
set of recursive structs). You will still need dynamic dispatch for some cases,
but using this method means that the number of cases where this is necessary is
very small.&lt;/p&gt;

&lt;h2 id=&#34;use-stack-based-variable-length-datatypes&#34;&gt;Use stack-based variable-length datatypes&lt;/h2&gt;

&lt;p&gt;Fixed-length datatypes are trivially storable on the stack, but for
dynamically-sized data it&amp;rsquo;s not so simple. However, &lt;a href=&#34;https://github.com/servo/rust-smallvec&#34;&gt;&lt;code&gt;smallvec&lt;/code&gt;&lt;/a&gt;,
&lt;a href=&#34;https://github.com/jFransham/smallstring&#34;&gt;&lt;code&gt;smallstring&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/servo/tendril&#34;&gt;&lt;code&gt;tendril&lt;/code&gt;&lt;/a&gt; are all variable-length
datatypes that allow you to store small numbers of elements on the
stack (shameless plug: &lt;code&gt;smallstring&lt;/code&gt; was written by me). Due to the law of small
numbers, you are very likely to have more of these small strings than larger
ones. This is good because it reduces allocation, but it&amp;rsquo;s &lt;em&gt;great&lt;/em&gt; if you&amp;rsquo;re
storing these in a &lt;code&gt;Vec&lt;/code&gt; or &lt;code&gt;HashMap&lt;/code&gt;, since you will have less indirection and
therefore better cache use. A good rule of thumb is to never have more than one
layer of pointers to dereference before you reach your value (NASA enforces this
rule in their C code, albeit for reliability and not performance).&lt;/p&gt;

&lt;p&gt;Libraries like &lt;code&gt;smallvec&lt;/code&gt; are great for cache locality, since an array of
&lt;code&gt;SmallVec&amp;lt;[T; 4]&amp;gt;&lt;/code&gt; will have exactly the same cache-locality as an array of just
&lt;code&gt;T&lt;/code&gt; - as long as the length of each &lt;code&gt;SmallVec&lt;/code&gt; is below 8 it just gets stored in
order. Going back to the cache-miss calculations from earlier:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// This is a gross oversimplification of how this type is implemented in the
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// crate, but it&amp;#39;s enough to explain how it works.
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;enum&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;SmallVec&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;T&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; {
    Small([T; &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]),
    Big(Vec&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;T&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;),
}

&lt;span style=&#34;color:#66d9ef&#34;&gt;type&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Matrix&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;T&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SmallVec&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;SmallVec&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;T&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As long as there are less than or equal to 4 elements in the &lt;code&gt;SmallVec&lt;/code&gt;, the
size of each instance is the size of the data plus the size of the tag, which
is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; size_of_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; size_of::&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;T&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;() &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;;
&lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; size_of_tag  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(size_of::&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;u8&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(), align_of::&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;T&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;());
size_of_data &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; size_of_tag&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The obvious question is why the size of the tag isn&amp;rsquo;t just &lt;code&gt;size_of::&amp;lt;u8&amp;gt;()&lt;/code&gt;.
This is because if &lt;code&gt;T&lt;/code&gt; was more than 1 byte in size, this would mean that all of
the elements would all be unaligned by 1 byte, which is bad. CPUs work much
slower on unaligned data, but unless you write a compiler you will never have to
think about that. The size of the data and its alignment don&amp;rsquo;t have to be the
same. For structs, for example, the alignment is typically the largest alignment
of any of its members. For primitive types like pointers, integers and floats
the alignment is the same as its size. The alignment and size of an &lt;code&gt;f32&lt;/code&gt; are
both 4. The alignment of a &lt;code&gt;SmallVec&amp;lt;f32&amp;gt;&lt;/code&gt; is the largest alignment of its
members, which is same as the alignment of &lt;code&gt;[f32; 4]&lt;/code&gt;, which is the same as the
alignment of &lt;code&gt;f32&lt;/code&gt;: 4.&lt;/p&gt;

&lt;p&gt;Consider we had a 4x4 matrix of &lt;code&gt;f32&lt;/code&gt;, this would mean that the size of the
matrix would be:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;Inner SmallVec size = 4 * 4 + 4
Matrix size = 4 * (4 * 4 + 4) + 4 = 84
Maximum 3 cache misses&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We don&amp;rsquo;t need to calculate the inner and outer cache misses seperately because
they are guaranteed to be next to each other in memory.&lt;/p&gt;

&lt;p&gt;From a cache standpoint this is as good as the flat vector representation, but
there&amp;rsquo;s nothing stopping you from accidentally making the inner vectors
different lengths and breaking the invariant that an array&amp;rsquo;s rows should be the
same length.&lt;/p&gt;

&lt;p&gt;I want to make something clear: you will never do these calculations in the
process of optimizing your program. This is merely some mathematical
justification for the voodoo folklore that &amp;ldquo;allocation is bad&amp;rdquo;, since that is
often countered by &amp;ldquo;&lt;code&gt;malloc&lt;/code&gt; is fast&amp;rdquo;. Both statements are true - the actual
process of allocating and deallocating memory is fast, but data structures that
allocate are worse for use-cases that require maximum speed.&lt;/p&gt;

&lt;h2 id=&#34;loop-unrolling-is-still-cool&#34;&gt;Loop unrolling is still cool&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Duff&#39;s_device&#34;&gt;Duff&amp;rsquo;s device&lt;/a&gt; is fun, but array-length-generic unrolled loops are
unlikely to be faster than the equivalent optimized nave code nowadays, since
any optimizing compiler worth its bits will do this kind of optimization without
having to mangle your code and ruining future-you&amp;rsquo;s day.&lt;/p&gt;

&lt;p&gt;Having said that, if you know that an array is likely to be a multiple of N
size, try making it a &lt;code&gt;&amp;amp;[[T; N]]&lt;/code&gt; and operating on a &lt;code&gt;[T; N]&lt;/code&gt; in each iteration.
This reduces the number of iterations (and therefore, the number of times you
need to recalculate the loop variables) and allows the compiler to operate more
aggressively on the loop body.&lt;/p&gt;

&lt;p&gt;You can also use more classical loop unrolling if it allows you to reduce the
&amp;ldquo;strength&amp;rdquo; of your operations. This means that if you have to calculate some
value for each iteration of the loop and calculating this value takes longer
than the body itself, manually unroll the body so you can calculate it less.
Example: you can implement an integer logarithm function like so:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;log_base&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; n: &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;, base: &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;) -&amp;gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt; {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;

    &lt;span style=&#34;color:#66d9ef&#34;&gt;loop&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; base { &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; out; }

        out &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;
        n &lt;span style=&#34;color:#f92672&#34;&gt;/=&lt;/span&gt; base;
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However, &lt;code&gt;n /= base; out += 1;&lt;/code&gt; is slower to calculate than &lt;code&gt;n &amp;lt; base&lt;/code&gt;. To take
advantage of this fact, you can unroll the loop like so:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;log_base_unrolled&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; n: &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;, base: &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;) -&amp;gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt; {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; UNROLL_COUNT: &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;;

    &lt;span style=&#34;color:#75715e&#34;&gt;// We use a fixed-size array to ensure that we don&amp;#39;t get the array count and
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// the `out` skip value out of sync.
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; premultiplied_base: [_; UNROLL_COUNT] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [
        base,
        base &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; base,
        base &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; base &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; base,
        base &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; base &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; base &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; base,
    ];

    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;loop&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; premultiplied_base[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] { &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; out; }
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; premultiplied_base[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] { &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; out &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;; }
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; premultiplied_base[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] { &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; out &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;; }
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; premultiplied_base[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] { &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; out &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;; }
        
        n &lt;span style=&#34;color:#f92672&#34;&gt;/=&lt;/span&gt; precalculated_base[UNROLL_COUNT &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;];
        out &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; UNROLL_COUNT;
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here are the benchmarks I used:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#[bench]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bench_log_base&lt;/span&gt;(b: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;mut&lt;/span&gt; Bencher) {
    b.iter(&lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#ae81ff&#34;&gt;5000000120510250&lt;/span&gt;);

        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(log_base(input, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;), &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;);
    });
}

&lt;span style=&#34;color:#75715e&#34;&gt;#[bench]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bench_log_base_unrolled&lt;/span&gt;(b: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;mut&lt;/span&gt; Bencher) {
    b.iter(&lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#ae81ff&#34;&gt;5000000120510250&lt;/span&gt;);

        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(log_base(input, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;), &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;);
    });
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;test::black_box&lt;/code&gt; is a magic function that prevents rustc and LLVM calculating
those function calls at compile-time and converting them into a constant, which
usually they would (actually, it&amp;rsquo;s not magic, it&amp;rsquo;s just some inline assembly
that doesn&amp;rsquo;t do anything, since neither rustc nor LLVM will try to optimize
anything that&amp;rsquo;s been accessed by inline assembly).&lt;/p&gt;

&lt;p&gt;This gives the following results:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;test bench_log_base          ... bench:  18 ns/iter (+/- 0)
test bench_log_base_unrolled ... bench:   5 ns/iter (+/- 0)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Wait a minute, though, what happens when we give a non-constant value for
&lt;code&gt;base&lt;/code&gt;?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#[bench]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bench_log_base_nonconstbase&lt;/span&gt;(b: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;mut&lt;/span&gt; Bencher) {
    b.iter(&lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#ae81ff&#34;&gt;5000000120510250&lt;/span&gt;);
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; base &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;);

        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(log_base(input, base), &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;);
    });
}

&lt;span style=&#34;color:#75715e&#34;&gt;#[bench]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bench_log_base_unrolled_nonconstbase&lt;/span&gt;(b: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;mut&lt;/span&gt; Bencher) {
    b.iter(&lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#ae81ff&#34;&gt;5000000120510250&lt;/span&gt;);
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; base &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;);

        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(log_base_unrolled(input, base), &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;);
    });
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;test bench_log_base_unrolled_nonconstbase ... bench:  37 ns/iter (+/- 1)
test bench_log_base_nonconstbase          ... bench: 199 ns/iter (+/- 5)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;They&amp;rsquo;re both much slower! Can we do better? Turns out yes, we can:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;log_base_increasing&lt;/span&gt;(n: &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;, base: &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;) -&amp;gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt; {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; UNROLL_COUNT: &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;;

    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; premultiplied_base: [_; UNROLL_COUNT] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [
        base,
        base &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; base,
        base &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; base &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; base,
        base &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; base &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; base &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; base,
    ];

    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; premultiplied_base[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] { &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;; }
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; premultiplied_base[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] { &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;; }
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; premultiplied_base[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] { &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;; }
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; premultiplied_base[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] { &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;; }

    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; UNROLL_COUNT &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; mul &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; premultiplied_base[UNROLL_COUNT &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;];

    &lt;span style=&#34;color:#66d9ef&#34;&gt;loop&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; premultiplied_base[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; mul { &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; out; }
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; premultiplied_base[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; mul { &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; out &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;; }
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; premultiplied_base[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; mul { &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; out &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;; }
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; premultiplied_base[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; mul { &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; out &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;; }

        mul &lt;span style=&#34;color:#f92672&#34;&gt;*=&lt;/span&gt; premultiplied_base[UNROLL_COUNT &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;];
        out &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; UNROLL_COUNT;
    }
}

&lt;span style=&#34;color:#75715e&#34;&gt;#[bench]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bench_log_base_increasing&lt;/span&gt;(b: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;mut&lt;/span&gt; Bencher) {
    b.iter(&lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#ae81ff&#34;&gt;5000000120510250&lt;/span&gt;);

        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(log_base_increasing(input, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;), &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;);
    });
}

&lt;span style=&#34;color:#75715e&#34;&gt;#[bench]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bench_log_base_increasing_nonconstbase&lt;/span&gt;(b: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;mut&lt;/span&gt; Bencher) {
    b.iter(&lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#ae81ff&#34;&gt;5000000120510250&lt;/span&gt;);
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; base &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;);

        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(log_base_increasing(input, base), &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;);
    });
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s check out the results now:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;test bench_log_base                         ... bench:  18 ns/iter (+/- 0)
test bench_log_base_nonconstbase            ... bench: 199 ns/iter (+/- 5)

test bench_log_base_unrolled                ... bench:   5 ns/iter (+/- 0)
test bench_log_base_unrolled_nonconstbase   ... bench:  37 ns/iter (+/- 1)

test bench_log_base_increasing              ... bench:   6 ns/iter (+/- 0)
test bench_log_base_increasing_nonconstbase ... bench:   8 ns/iter (+/- 1)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Turns out the compiler was doing something sneaky: it can optimize integer
division by a constant &lt;a href=&#34;http://embeddedgurus.com/stack-overflow/2009/06/division-of-integers-by-constants/&#34;&gt;into a multiplication combined with a shift&lt;/a&gt;.
When it could no longer fold the constant into the function it slowed down
considerably. It&amp;rsquo;s ok to rely on const-folding if it allows you to gain
considerable speedups and you know that the function will usually be called with
constant arguments, but be careful. The things to look out for are if statements
and integer division, both of which can be much slower with non-constant values
compared to constants.&lt;/p&gt;

&lt;p&gt;The fastest method by far converts to an &lt;code&gt;f64&lt;/code&gt;, calls &lt;code&gt;.log(base)&lt;/code&gt; on that, and
then converts back. It doesn&amp;rsquo;t work for large numbers, however, because of loss
of precision. This is probably a good time to note that although adding and
multiplying integers is faster than doing the same for floats, for code that
does division by a non-constant value or something more complex like
trigonometry, you should definitely use floats. The compiler can&amp;rsquo;t do the
conversion for you - it won&amp;rsquo;t apply optimizations that make your code less
precise - but you can check for areas where this would be an improvement and
make the change manually.&lt;/p&gt;

&lt;h2 id=&#34;assert-conditions-beforehand&#34;&gt;&lt;code&gt;assert!&lt;/code&gt; conditions beforehand&lt;/h2&gt;

&lt;p&gt;If you want to reduce the number of implicit asserts that get compiled into the
code, then instead of this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;do_something_with_array&lt;/span&gt;(array: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;[&lt;span style=&#34;color:#66d9ef&#34;&gt;u8&lt;/span&gt;]) -&amp;gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;u8&lt;/span&gt; {
    array[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; array[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; array[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; array[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; array[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; array[&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Do this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;do_something_with_array&lt;/span&gt;(array: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;[&lt;span style=&#34;color:#66d9ef&#34;&gt;u8&lt;/span&gt;]) -&amp;gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;u8&lt;/span&gt; {
    assert&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(array.len &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;);
    array[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; array[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; array[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; array[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; array[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; array[&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This allows LLVM to realize that the later asserts are unreachable and elides
them. This is useful for any code that may assert multiple different qualities
about the same data, but is especially useful for indexing since we know that
if &lt;code&gt;array[n]&lt;/code&gt; succeeds then &lt;code&gt;array[n - 1]&lt;/code&gt; will succeed too. This is similar to
the point about fixed-length arrays in the previous section.&lt;/p&gt;

&lt;p&gt;Essentially, try to consolidate checks into a single &lt;code&gt;assert!&lt;/code&gt;. This means that
the later checks become statically unreachable. If LLVM/Rust still don&amp;rsquo;t
optimize it away you can switch to using the unsafe indexing methods while
ensuring that they&amp;rsquo;re still safe. This tip is shamelessly stolen from &lt;a href=&#34;https://www.reddit.com/r/rust/comments/6anp0d/suggestion_for_a_new_rustc_optimization/dhfzp93/&#34;&gt;a comment
on /r/rust&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;use-link-time-optimization&#34;&gt;Use link-time optimization&lt;/h2&gt;

&lt;p&gt;Normally, Rust can only inline functions that are either defined in-crate or,
in the case of functions in other libraries, have &lt;code&gt;#[inline]&lt;/code&gt; specified. LTO
allows the compiler to inline cross-crate, at the cost of a compile-time speed
penalty. I am of the opinion that compile times only matter for debug builds, so
that&amp;rsquo;s a tradeoff I&amp;rsquo;m willing to make. As with everything else here, profile and
check that the tradeoff is worthwhile.&lt;/p&gt;

&lt;h2 id=&#34;don-t-use-inline-always&#34;&gt;Don&amp;rsquo;t use &lt;code&gt;#[inline(always)]&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;#[inline(always)]&lt;/code&gt; feels good as a performance hint, but the truth is that
optimizing compilers are really good at working out when a function would
benefit from being inlined, and Rust isn&amp;rsquo;t constrained to the slower
standardized C calling convention and can use &lt;code&gt;fastcc&lt;/code&gt;, making function calls
extremely cheap. You&amp;rsquo;re more likely to cause the size of your executable to
bloat. This takes up more space on your hard drive, of course, but that&amp;rsquo;s not
too much of a problem. If you have even a single bundled asset like images or
audio they will likely dwarf the size of your executable.&lt;/p&gt;

&lt;p&gt;The real issue here is that it can make your program no longer fit in the CPU&amp;rsquo;s
instruction cache. The CPU will only have to go to RAM for its instructions when
functions are called with instructions outside of the current cache line. The
larger your binary is, the more likely this is, and the more functions are
inlined, the larger your binary is. It&amp;rsquo;s not the end of the world to have a
large binary, but unless you&amp;rsquo;re really certain that something will be improved
by manually marking it as inline, and you have benchmarks to back that up,
it&amp;rsquo;s just as likely that you&amp;rsquo;ll slow a program down with careless inlining than
to speed it up.&lt;/p&gt;

&lt;p&gt;So now I&amp;rsquo;ve scared you off inlining, let&amp;rsquo;s talk about when you should explicitly
add inlining annotations. Small functions that are called often are a good
target for inlining. &lt;code&gt;Iterator::next&lt;/code&gt;, for example, or &lt;code&gt;Deref::deref&lt;/code&gt;. The
overhead from calling these functions may be larger than the time it takes to
run the function itself. These are likely to be automatically inlined when
called internally, but marking these as &lt;code&gt;#[inline]&lt;/code&gt; will allow users of your
library to inline them too, even if they don&amp;rsquo;t use LTO. Only functions marked
&lt;code&gt;#[inline]&lt;/code&gt; will be considered for cross-crate inlining, but that means the
definition has to be stored in the compiled library, causing bloat and
increasing compile times. &lt;code&gt;#[inline(always)]&lt;/code&gt; is even more niche, but it&amp;rsquo;s
sometimes nice to ensure that a tiny function will be inlined, or as a kind of
documentation that the function call is free for if someone comes along and
tries to manually inline it to improve performance. It really is very rare that
you would want to do this, though, and it&amp;rsquo;s best to just trust the compiler.&lt;/p&gt;

&lt;p&gt;The other class of functions that are good targets for annotating inlining are
ones that you know to often be called with constant parameters. We go into this
later on, but &lt;code&gt;{integer}::from_str_radix&lt;/code&gt; is an exellent example of this. Most
uses of this function will have a constant as the second parameter, and so by
judicious use of &lt;code&gt;#[inline]&lt;/code&gt; we can prevent branching and expensive operations
like division for the consumers of our library. It&amp;rsquo;s not worth losing sleep
over though, since they could just use link-time optimization if they need to
squeeze out every last drop of performance.&lt;/p&gt;

&lt;p&gt;Also, the compiler does really get it wrong sometimes, and can miss out on
inlining opportunities that would improve code speed. However, only add
&lt;code&gt;#[inline(always)]&lt;/code&gt; annotation if you can prove with benchmarks that it improves
the speed, and adding these annotations is a bit of a dark art. You effort is
probably better spent elsewhere.&lt;/p&gt;

&lt;p&gt;If you want to reduce the size of your code, you can try using
&lt;code&gt;panic = &amp;quot;abort&amp;quot;&lt;/code&gt;. This removes the &amp;ldquo;landing pads&amp;rdquo; that allow Rust to show a
nice stack trace after a panic, and causes any panic to end the program
instantly. I have legitimately seen non-trivial speedups on benchmarks for the
&lt;code&gt;ion&lt;/code&gt; shell after adding this option to the release build, and I can only
attribute it to making more code fit in the instruction cache. I have not tried
it with many other programs, but it would probably only affect medium to large
projects. Try it out on your codebase, it&amp;rsquo;s as easy as adding one line to the
&lt;code&gt;Cargo.toml&lt;/code&gt; and it may improve your code&amp;rsquo;s speed.&lt;/p&gt;

&lt;h2 id=&#34;parallelize-but-not-how-you-think&#34;&gt;Parallelize, but not how you think&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;s an absolutely amazing library for Haskell called &lt;a href=&#34;https://github.com/facebook/haxl&#34;&gt;Haxl&lt;/a&gt;, that
automatically tracks the data dependencies of your network requests and batches
them and runs them asynchronously as long as they don&amp;rsquo;t overlap. It&amp;rsquo;s something
that shows the power of computational abstractions like monads and it&amp;rsquo;s not
something that has a, ahem, &lt;em&gt;parallel&lt;/em&gt; in any other language, as far as I know.
At least, not for IO. We&amp;rsquo;ve had this exact ability in the CPU for a long, long
time. The CPU tracks the data dependencies of computations and will parallelize
them wherever possible.&lt;/p&gt;

&lt;p&gt;The reason data dependencies matter is that the CPU doesn&amp;rsquo;t just execute one
instruction at a time. As long as two instructions don&amp;rsquo;t share a register they
can safely be run simultaneously, so the CPU does so. This is essentially free
parallelism without the need for locks, work queues or anything that affects
your architecture at all, so you would be crazy not to take advantage of it.&lt;/p&gt;

&lt;p&gt;Parallelizable computation also lends itself well to autovectorization, which is
the process where the compiler realizes that you&amp;rsquo;re doing the same thing to
multiple different values and converts it to a special instruction that, well,
does the same thing to multiple different values.&lt;/p&gt;

&lt;p&gt;For example, the compiler could translate the following numerical code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;(a1 &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; a2) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (b1 &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b2) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (c1 &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; c2) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (d1 &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; d2)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;into just one instruction that executes all four subexpressions as fast as a
single addition.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;%intermediate-value = add-vectors [%a1 %b1 %c1 %d1] [%a2 %b2 %c2 %d2]
sum-parts %intermediate-value&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;a-case-study&#34;&gt;A case study&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s write a version of &lt;code&gt;usize::from_str_radix&lt;/code&gt; that&amp;rsquo;s about 30% faster than
the one in the standard library.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// We&amp;#39;re redefining these here since they&amp;#39;re private in the stdlib
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#[derive(Debug, Clone, PartialEq, Eq)]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ParseIntError&lt;/span&gt; {
    kind: &lt;span style=&#34;color:#a6e22e&#34;&gt;IntErrorKind&lt;/span&gt;,
}

&lt;span style=&#34;color:#75715e&#34;&gt;#[derive(Debug, Clone, PartialEq, Eq)]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;enum&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;IntErrorKind&lt;/span&gt; {
    Empty,
    InvalidDigit,
    Overflow,
    Underflow,
}

&lt;span style=&#34;color:#75715e&#34;&gt;#[inline]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;from_str_radix&lt;/span&gt;(input: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;str&lt;/span&gt;, radix: &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;) -&amp;gt; Result&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;, ParseIntError&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;to_digit_ascii&lt;/span&gt;(ascii: &lt;span style=&#34;color:#66d9ef&#34;&gt;u8&lt;/span&gt;, radix: &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;) -&amp;gt; Result&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;, ParseIntError&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; decimal_digit &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ascii.wrapping_sub(&lt;span style=&#34;color:#e6db74&#34;&gt;b&amp;#39;0&amp;#39;&lt;/span&gt;);

        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; radix &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; decimal_digit &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt; {
            &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (ascii &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;).wrapping_sub(&lt;span style=&#34;color:#e6db74&#34;&gt;b&amp;#39;a&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;;

            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; out &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; radix &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; {
                Err(ParseIntError { kind: &lt;span style=&#34;color:#a6e22e&#34;&gt;IntErrorKind&lt;/span&gt;::InvalidDigit })
            } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
                Ok(out &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)
            }
        } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
            &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; decimal_digit &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; decimal_digit &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;;
            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; decimal_digit &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; radix {
                Err(ParseIntError { kind: &lt;span style=&#34;color:#a6e22e&#34;&gt;IntErrorKind&lt;/span&gt;::InvalidDigit })
            } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
                Ok(decimal_digit)
            }
        }
    }

    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; radix &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;36&lt;/span&gt; {
        panic&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;from_str_radix: radix is too high (maximum 36)&amp;#34;&lt;/span&gt;);
    }

    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; bytes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input.as_bytes();

    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; bytes.len() &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; Err(
            ParseIntError { kind: &lt;span style=&#34;color:#a6e22e&#34;&gt;IntErrorKind&lt;/span&gt;::Empty }
        );
    }

    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; bytes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;match&lt;/span&gt; bytes[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] {
        &lt;span style=&#34;color:#e6db74&#34;&gt;b&amp;#39;+&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;bytes[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;..] },
        &lt;span style=&#34;color:#e6db74&#34;&gt;b&amp;#39;-&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; Err(ParseIntError { kind: &lt;span style=&#34;color:#a6e22e&#34;&gt;IntErrorKind&lt;/span&gt;::Underflow }) },
        _ &lt;span style=&#34;color:#f92672&#34;&gt;=&amp;gt;&lt;/span&gt; bytes,
    };

    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; mul &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; radix;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bytes.len() &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;

    &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; to_digit_ascii(bytes[index], radix)&lt;span style=&#34;color:#f92672&#34;&gt;?&lt;/span&gt;;

    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;byte &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; bytes[..index].iter().rev() {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; digit &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; to_digit_ascii(byte, radix)&lt;span style=&#34;color:#f92672&#34;&gt;?&lt;/span&gt;;

        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; next_output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; output.wrapping_add(digit &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; mul);

        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; output &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; next_output {
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; Err(
                ParseIntError { kind: &lt;span style=&#34;color:#a6e22e&#34;&gt;IntErrorKind&lt;/span&gt;::Overflow }
            );
        }

        mul &lt;span style=&#34;color:#f92672&#34;&gt;*=&lt;/span&gt; radix;
        output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; next_output;
    }

    Ok(output)
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I explicitly use &lt;code&gt;wrapping_*&lt;/code&gt; functions not for optimization purposes (because
overflow checks are removed at runtime), but because overflow is required for
correct behaviour. You&amp;rsquo;ll notice some optimizations here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We start at the end and work backwards, keeping a &amp;ldquo;mul&amp;rdquo; counter. Originally I
wrote a version of this that works forwards and multiplies &lt;code&gt;output&lt;/code&gt; by radix
each loop but the backwards method is 10% faster. This seems to be due to
better instruction-level parallelism. The multiplications can be parallelized
and only the addition relies on the previous iteration&amp;rsquo;s value for &lt;code&gt;output&lt;/code&gt;,
and addition is much faster.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Any folding operations can be improved in this way by exploiting the
  algebraic laws (in this case, the distributive law) to improve the number of
  operations that can be done in parallel.
* We rely on overflow to test &lt;code&gt;A &amp;lt; x &amp;lt; B&lt;/code&gt; comparisons. This is only useful here
  because we already need the &lt;code&gt;x - A&lt;/code&gt; value, so we&amp;rsquo;re saving an extra comparison
  and a bitwise and. In most code &lt;code&gt;A &amp;lt; x &amp;amp;&amp;amp; x &amp;lt; B&lt;/code&gt; is as cheap or cheaper than
  &lt;code&gt;x - A &amp;lt; B&lt;/code&gt; with overflow.
* We use &lt;code&gt;| 32&lt;/code&gt; to unify the codepaths for upper- and lowercase letters,
  reducing the number of comparisons we need to do.
* We don&amp;rsquo;t do &lt;code&gt;output + digit * mul&lt;/code&gt; when &lt;code&gt;output == 0&lt;/code&gt; and &lt;code&gt;mul == 1&lt;/code&gt;. This
  seems to be consistently 1ns faster, but it&amp;rsquo;s possible that this doesn&amp;rsquo;t make
  a difference and the 1ns speedup I&amp;rsquo;m seeing is pure luck. I reran the
  benchmarks both with and without the change multiple times and saw a
  consistent difference but this doesn&amp;rsquo;t rule out luck. This is the problem with
  microbenchmarks, when the difference becomes small enough you can&amp;rsquo;t tell
  whether you&amp;rsquo;re really making it faster.
* We use Rust&amp;rsquo;s safe iterator interface, which is as fast as the C idiom of
  storing a &amp;ldquo;start&amp;rdquo; and &amp;ldquo;end&amp;rdquo; pointer so you can check whether the loop is
  finished with a simple &lt;code&gt;==&lt;/code&gt;. If you ever hear someone say &amp;ldquo;Rust&amp;rsquo;s safety
  guarantees are useless because you need to drop down to unsafe to get any real
  speed&amp;rdquo; (I&amp;rsquo;ve seen this almost verbatim on Hacker News before) you can show
  them this.
* We don&amp;rsquo;t rely on const-folding in order to make our code fast, but it does run
  faster with a constant value for &lt;code&gt;radix&lt;/code&gt;. Therefore, we add &lt;code&gt;#[inline]&lt;/code&gt; to
  allow downstream crates to apply const-folding too.&lt;/p&gt;

&lt;p&gt;The method I used for this is the basic method you should use for any
optimization work: write a representative benchmark and then progressively tweak
and rerun benchmarks until you can&amp;rsquo;t shave off any more cycles. Doing this for
pure functions is much easier, so one of the first things you should do to
optimize any function that&amp;rsquo;s called in a tight loop is to make it pure. This
avoids indirect writes and reads (reads and writes to places in memory that are
likely to be outside cache lines) and makes benchmarking much, much easier. If
you use test-driven development for reliability, this is the equivalent for
performance.&lt;/p&gt;

&lt;p&gt;Extending this to work on signed integer types is an exercise for the reader.
Tip: unlike C, you can rely on signed integers overflowing with 2&amp;rsquo;s complement
arithmetic.&lt;/p&gt;

&lt;p&gt;Here are the functions I used to benchmark the code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#[bench]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bench_from_str&lt;/span&gt;(b: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;mut&lt;/span&gt; Bencher) {
    b.iter(&lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1235112512&amp;#34;&lt;/span&gt;);
        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(from_str_radix(input, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;), Ok(&lt;span style=&#34;color:#ae81ff&#34;&gt;1235112512&lt;/span&gt;));
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;FFaf125A&amp;#34;&lt;/span&gt;);
        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(from_str_radix(input, &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;), Ok(&lt;span style=&#34;color:#ae81ff&#34;&gt;0xFFaf125A&lt;/span&gt;));
    });
}

&lt;span style=&#34;color:#75715e&#34;&gt;#[bench]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bench_from_str_native&lt;/span&gt;(b: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;mut&lt;/span&gt; Bencher) {
    b.iter(&lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1235112512&amp;#34;&lt;/span&gt;);
        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;::from_str_radix(input, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;), Ok(&lt;span style=&#34;color:#ae81ff&#34;&gt;1235112512&lt;/span&gt;));
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;FFaf125A&amp;#34;&lt;/span&gt;);
        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;::from_str_radix(input, &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;), Ok(&lt;span style=&#34;color:#ae81ff&#34;&gt;0xFFaf125A&lt;/span&gt;));
    });
}

&lt;span style=&#34;color:#75715e&#34;&gt;#[bench]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bench_from_str_nonconstradix&lt;/span&gt;(b: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;mut&lt;/span&gt; Bencher) {
    b.iter(&lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1235112512&amp;#34;&lt;/span&gt;);
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; radix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;);
        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(from_str_radix(input, radix), Ok(&lt;span style=&#34;color:#ae81ff&#34;&gt;1235112512&lt;/span&gt;));
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;FFaf125A&amp;#34;&lt;/span&gt;);
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; radix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;);
        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(from_str_radix(input, radix), Ok(&lt;span style=&#34;color:#ae81ff&#34;&gt;0xFFaf125A&lt;/span&gt;));
    });
}

&lt;span style=&#34;color:#75715e&#34;&gt;#[bench]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bench_from_str_native_nonconstradix&lt;/span&gt;(b: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;mut&lt;/span&gt; Bencher) {
    b.iter(&lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1235112512&amp;#34;&lt;/span&gt;);
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; radix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;);
        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;::from_str_radix(input, radix), Ok(&lt;span style=&#34;color:#ae81ff&#34;&gt;1235112512&lt;/span&gt;));
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;FFaf125A&amp;#34;&lt;/span&gt;);
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; radix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;);
        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;::from_str_radix(input, radix), Ok(&lt;span style=&#34;color:#ae81ff&#34;&gt;0xFFaf125A&lt;/span&gt;));
    });
}

&lt;span style=&#34;color:#75715e&#34;&gt;#[bench]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bench_from_str_1char&lt;/span&gt;(b: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;mut&lt;/span&gt; Bencher) {
    b.iter(&lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;);
        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(from_str_radix(input, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;), Ok(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;));
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;F&amp;#34;&lt;/span&gt;);
        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(from_str_radix(input, &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;), Ok(&lt;span style=&#34;color:#ae81ff&#34;&gt;0xF&lt;/span&gt;));
    });
}

&lt;span style=&#34;color:#75715e&#34;&gt;#[bench]&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bench_from_str_native_1char&lt;/span&gt;(b: &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;mut&lt;/span&gt; Bencher) {
    b.iter(&lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;);
        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;::from_str_radix(input, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;), Ok(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;));
        &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; black_box(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;F&amp;#34;&lt;/span&gt;);
        assert_eq&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;usize&lt;/span&gt;::from_str_radix(input, &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;), Ok(&lt;span style=&#34;color:#ae81ff&#34;&gt;0xF&lt;/span&gt;));
    });
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Results:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;test bench_from_str                      ... bench:          22 ns/iter (+/- 7)
test bench_from_str_native               ... bench:          36 ns/iter (+/- 0)
test bench_from_str_nonconstradix        ... bench:          26 ns/iter (+/- 0)
test bench_from_str_native_nonconstradix ... bench:          39 ns/iter (+/- 0)
test bench_from_str_1char                ... bench:           5 ns/iter (+/- 0)
test bench_from_str_native_1char         ... bench:          13 ns/iter (+/- 0)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Something I&amp;rsquo;ve noticed with benchmarks below 1ms is that it can take some time
to &amp;ldquo;spin up&amp;rdquo; the CPU. Occasionally the first benchmark in a set will take
20-30ns longer than the ones after it. If you duplicate the benchmark verbatim
and take the number with the lowest variance this avoids the issue. I think this
is due to the CPU needing to gather information in order to do proper branch
prediction. Ideally you&amp;rsquo;d just not do micro-benchmarks, but some functions do
legitimately call for it. Don&amp;rsquo;t trust a benchmark, especially a microbenchmark,
until you&amp;rsquo;ve rerun it multiple times.&lt;/p&gt;

&lt;p&gt;When I reran this particular benchmark (at least 10 times in total, not
including the benchmarks I ran while editing the code) to ensure that the
numbers were stable, and although the averages are extremely stable (the native
one sometimes was slightly slower, the 36ns value above is what I see most of
the time), the variances are mostly 0-3ns with spikes of 13-26ns. I don&amp;rsquo;t have a
good explanation for this, expect a follow-up post with tips on writing better
benchmarks.&lt;/p&gt;

&lt;p&gt;This is a perfect example of why low-level optimization is important, since this
is exactly the kind of function that could be used hundreds of thousands of
times in parsers of textual data and a 10ns speedup here could lead to
meaningful improvements over the life of the program. It&amp;rsquo;s also an example of
why you should avoid low-level optimization when possible. The original stdlib
implementation of this function isn&amp;rsquo;t the most idiomatic code, but it&amp;rsquo;s
significantly more readable than this. Having said that, though, it&amp;rsquo;s a
testament to Rust&amp;rsquo;s commitment to zero-cost abstractions that you can write
mostly-idiomatic, safe code and have it perform as well as equivalent C/C++ code
that would require use of unsafe pointer arithmetic.&lt;/p&gt;

&lt;h2 id=&#34;wrapping-up&#34;&gt;Wrapping up&lt;/h2&gt;

&lt;p&gt;If I had to sum the trick to optimization up in a pithy QotW-ready snippet it
would be this:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The fastest code is code that doesn&amp;rsquo;t run at all, the second-fastest code is
code that never stops running.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Ideally, you want to do less work, and if you&amp;rsquo;re doing the minimum amount of
work you want to reduce the amount of time the CPU spends waiting around.&lt;/p&gt;

&lt;p&gt;If you want more Rustic performance tips with more numbers I would consider
&lt;a href=&#34;http://blog.burntsushi.net/ripgrep/&#34;&gt;BurntSushi&amp;rsquo;s excellent post-mortem of ripgrep&lt;/a&gt; required
reading for anyone wanting to write fast software (it was the thing that
originally sent me down this deep, deep rabbit hole). For more general systems
language-y points, check out Andrei Alexandrescu&amp;rsquo;s talk &lt;a href=&#34;https://www.youtube.com/watch?v=o4-CwDo2zpg&#34;&gt;&amp;ldquo;Fastware&amp;rdquo;&lt;/a&gt;,
from which the &lt;code&gt;from_str_radix&lt;/code&gt; and &lt;code&gt;log_base&lt;/code&gt; code was adapted. A lot of the
points in this article are expansions upon points behind one of those two links.&lt;/p&gt;

&lt;p&gt;I hope that whether you&amp;rsquo;re a soft-shell Rustacean or a grizzled veteran, this
has given you a better sense of when some code may be poorly performing, and
what to do about it. Go make things go vroom.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>